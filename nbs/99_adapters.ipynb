{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Adapters\n",
    "\n",
    "> Adapter functions for the test suite, allowing easy instantiation and use of the BPE training and Tokenizer functionalities within test environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Attempt to import from local modules, will be available after nbdev_export\n",
    "try:\n",
    "    from lm.bpe_training import train_bpe\n",
    "    from lm.bpe_tokenizer_class import Tokenizer\n",
    "except ImportError:\n",
    "    # Fallback for direct notebook execution or if package not fully installed\n",
    "    # This helps in iterative development but assumes a certain project structure\n",
    "    # or that the user will run nbdev_export before tests that use these adapters.\n",
    "    print(\"Warning: Could not import from .bpe_training or .bpe_tokenizer_class. \\n\"\n",
    "          \"Ensure these modules are exported (e.g., via `nbdev_export`) before using adapters extensively outside of tests run with nbdev/pytest.\")\n",
    "    # Define dummy versions if imports fail, so the notebook can still be processed by nbdev\n",
    "    # However, actual tests requiring these will fail until real modules are available.\n",
    "    def train_bpe(input_path: str, vocab_size: int, special_tokens: List[str]) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "        raise NotImplementedError(\"train_bpe is not available. Please ensure '02_bpe_training.ipynb' is exported.\")\n",
    "    \n",
    "    class Tokenizer:\n",
    "        def __init__(self, vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], special_tokens: Optional[List[str]] = None):\n",
    "            raise NotImplementedError(\"Tokenizer class is not available. Please ensure '03_bpe_tokenizer_class.ipynb' is exported.\")\n",
    "        @classmethod\n",
    "        def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: Optional[List[str]] = None) -> 'Tokenizer':\n",
    "            raise NotImplementedError(\"Tokenizer.from_files is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_train_bpe_adapter(\n",
    "    input_path: str, \n",
    "    vocab_size: int, \n",
    "    special_tokens: List[str]\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"Adapter for the `train_bpe` function.\n",
    "    \n",
    "    This function directly calls `train_bpe` with the provided arguments and returns its output.\n",
    "    It's intended for use in test suites to easily invoke the BPE training process.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the raw text file for training.\n",
    "        vocab_size (int): Desired final vocabulary size.\n",
    "        special_tokens (List[str]): List of special token strings.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]: \n",
    "            The learned vocabulary and merge rules from `train_bpe`.\n",
    "    \"\"\"\n",
    "    return train_bpe(input_path, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_tokenizer_from_files_adapter(\n",
    "    vocab_path: str, \n",
    "    merges_path: str, \n",
    "    special_tokens: Optional[List[str]] = None\n",
    ") -> Tokenizer:\n",
    "    \"\"\"Adapter for the `Tokenizer.from_files` classmethod.\n",
    "\n",
    "    This function calls `Tokenizer.from_files` to load a tokenizer instance\n",
    "    from serialized vocabulary and merge files.\n",
    "\n",
    "    Args:\n",
    "        vocab_path (str): Path to the JSON file containing the vocabulary.\n",
    "        merges_path (str): Path to the JSON file containing the merge rules.\n",
    "        special_tokens (Optional[List[str]], optional): List of special token strings.\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tokenizer: An instance of the Tokenizer class.\n",
    "    \"\"\"\n",
    "    return Tokenizer.from_files(vocab_path, merges_path, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_tokenizer_adapter(\n",
    "    vocab: Dict[int, bytes], \n",
    "    merges: List[Tuple[bytes, bytes]], \n",
    "    special_tokens: Optional[List[str]] = None\n",
    ") -> Tokenizer:\n",
    "    \"\"\"Adapter for the `Tokenizer` constructor.\n",
    "\n",
    "    This function directly instantiates the `Tokenizer` class with the provided\n",
    "    vocabulary, merge rules, and special tokens.\n",
    "\n",
    "    Args:\n",
    "        vocab (Dict[int, bytes]): Vocabulary mapping token IDs to byte sequences.\n",
    "        merges (List[Tuple[bytes, bytes]]): List of BPE merge rules.\n",
    "        special_tokens (Optional[List[str]], optional): List of special token strings.\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tokenizer: An instance of the Tokenizer class.\n",
    "    \"\"\"\n",
    "    return Tokenizer(vocab, merges, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Example Usage (for illustration, actual testing would be in separate test files)\n",
    "if __name__ == '__main__':\n",
    "    import tempfile\n",
    "    import os\n",
    "    from lm.utils import save_tokenizer_components # Assuming utils are exported\n",
    "\n",
    "    print(\"Testing adapter functions (requires exported modules and dummy data)...\")\n",
    "\n",
    "    # Dummy data for testing adapters\n",
    "    dummy_vocab = {i: bytes([i]) for i in range(256)}\n",
    "    dummy_vocab[256] = b\"<EOS>\"\n",
    "    dummy_merges = [(b'e', b's')] \n",
    "    dummy_special_tokens = [\"<EOS>\"]\n",
    "\n",
    "    # Test get_tokenizer_adapter\n",
    "    try:\n",
    "        tokenizer_instance = get_tokenizer_adapter(dummy_vocab, dummy_merges, dummy_special_tokens)\n",
    "        assert isinstance(tokenizer_instance, Tokenizer), \"get_tokenizer_adapter failed to return Tokenizer instance\"\n",
    "        print(\"get_tokenizer_adapter: PASSED\")\n",
    "    except Exception as e:\n",
    "        print(f\"get_tokenizer_adapter: FAILED - {e}\")\n",
    "\n",
    "    # Setup for file-based adapters\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    vocab_file = os.path.join(temp_dir, \"dummy_vocab.json\")\n",
    "    merges_file = os.path.join(temp_dir, \"dummy_merges.json\")\n",
    "\n",
    "    try:\n",
    "        save_tokenizer_components(dummy_vocab, dummy_merges, vocab_file, merges_file)\n",
    "        \n",
    "        # Test get_tokenizer_from_files_adapter\n",
    "        tokenizer_from_files = get_tokenizer_from_files_adapter(vocab_file, merges_file, dummy_special_tokens)\n",
    "        assert isinstance(tokenizer_from_files, Tokenizer), \"get_tokenizer_from_files_adapter failed\"\n",
    "        print(\"get_tokenizer_from_files_adapter: PASSED\")\n",
    "\n",
    "        # Test run_train_bpe_adapter (basic call, requires a dummy input file)\n",
    "        dummy_input_text_file = os.path.join(temp_dir, \"dummy_input.txt\")\n",
    "        with open(dummy_input_text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"test text for bpe training est est.\")\n",
    "        \n",
    "        trained_vocab, trained_merges = run_train_bpe_adapter(dummy_input_text_file, 260, [\"<EOS>\"])\n",
    "        assert isinstance(trained_vocab, dict) and isinstance(trained_merges, list), \"run_train_bpe_adapter failed\"\n",
    "        print(f\"run_train_bpe_adapter: PASSED (vocab_size={len(trained_vocab)}, merges={len(trained_merges)})\")\n",
    "\n",
    "    except NotImplementedError as e:\n",
    "        print(f\"Adapter tests skipped due to missing modules: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Adapter tests FAILED during file operations: {e}\")\n",
    "    finally:\n",
    "        # Cleanup temporary directory and files\n",
    "        if os.path.exists(dummy_input_text_file):\n",
    "            os.remove(dummy_input_text_file)\n",
    "        if os.path.exists(vocab_file):\n",
    "            os.remove(vocab_file)\n",
    "        if os.path.exists(merges_file):\n",
    "            os.remove(merges_file)\n",
    "        if os.path.exists(temp_dir):\n",
    "            os.rmdir(temp_dir)\n",
    "    print(\"Adapter function tests completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
