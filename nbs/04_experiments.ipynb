{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "> Notebook for running training, experiments, and analysis. This includes dataset acquisition, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# --- Project Root Setup --- \n",
    "project_root = Path(os.getcwd())\n",
    "# Check if running from nbs/\n",
    "if project_root.name == 'nbs' and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "elif not (project_root / 'settings.ini').exists():\n",
    "     # Try going up one level if settings.ini not found directly\n",
    "     if (project_root.parent / 'settings.ini').exists():\n",
    "          project_root = project_root.parent\n",
    "     else:\n",
    "          # Heuristic: if 'nbs' is in the path, go up one level from 'nbs'\n",
    "          current_parts = project_root.parts\n",
    "          if 'nbs' in current_parts:\n",
    "            nbs_index = current_parts.index('nbs')\n",
    "            project_root = Path(*current_parts[:nbs_index])\n",
    "          if not (project_root / 'settings.ini').exists(): # Final check\n",
    "            print(\"Warning: Could not automatically determine project root based on 'settings.ini' or 'nbs' dir. Assuming current dir or its parent might be it, or sys.path is already correct.\")\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "if project_root_str not in sys.path:\n",
    "    # print(f\"Adding project root to sys.path: {project_root_str}\") # Verbose option\n",
    "    sys.path.insert(0, project_root_str)\n",
    "    # Also add the library path if it's distinct and not just project_root/lib_name\n",
    "    # Assuming settings.ini is at project_root and lib_path is specified relative to it\n",
    "    try:\n",
    "        import configparser\n",
    "        config = configparser.ConfigParser()\n",
    "        settings_file = project_root / 'settings.ini'\n",
    "        if settings_file.exists():\n",
    "            config.read(settings_file)\n",
    "            lib_name = config['DEFAULT'].get('lib_name', 'lm') # Default to 'lm' if not found\n",
    "            lib_path_str = str((project_root / lib_name).resolve()) # Assumes lib is directly under project root\n",
    "            if lib_path_str not in sys.path:\n",
    "                # print(f\"Adding library path to sys.path: {lib_path_str}\") # Verbose\n",
    "                sys.path.insert(0, lib_path_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Could not parse settings.ini to add specific lib_path: {e}\")\n",
    "else:\n",
    "    # print(f\"Project root already in sys.path: {project_root_str}\") # Less verbose\n",
    "    pass\n",
    "# --- End Project Root Setup --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported from 'bpe_tokenizer_scratch' library modules.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Iterable, Dict, List, Any # Added List, Any\n",
    "import tqdm\n",
    "import time # For measuring execution time\n",
    "import tempfile # For quick verification test\n",
    "\n",
    "# Try importing from the current project's structure first\n",
    "try:\n",
    "    from lm.bpe_training import train_bpe\n",
    "    from lm.utils import save_tokenizer_components, load_tokenizer_components\n",
    "    from lm.bpe_tokenizer_class import Tokenizer \n",
    "    print(\"Successfully imported from 'bpe_tokenizer_scratch' library modules.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import from 'bpe_tokenizer_scratch' library. Attempting fallback. Error: {e}\")\n",
    "    # Fallback if nbdev_export hasn't been run or if in a different environment\n",
    "    try:\n",
    "        # This fallback might be for when running nbs/04_experiments.ipynb directly \n",
    "        # and the main library `bpe_tokenizer_scratch` is in the parent directory.\n",
    "        # The project root setup above should ideally handle this.\n",
    "        from lm.bpe_training import train_bpe # Assuming direct import from exported .py in same dir (less likely with nbdev)\n",
    "        from lm.utils import save_tokenizer_components, load_tokenizer_components\n",
    "        from lm.bpe_tokenizer_class import Tokenizer \n",
    "        print(\"Successfully imported from local .py files (fallback). Ensure nbdev_export has been run for proper library structure.\")\n",
    "    except ImportError as e2:\n",
    "        print(f\"Fallback import also failed. Error: {e2}\")\n",
    "        print(\"Please ensure nbdev_export has been run and the library 'bpe_tokenizer_scratch' is in your PYTHONPATH or accessible.\")\n",
    "        # Dummy placeholders if imports fail, to allow notebook to load\n",
    "        def train_bpe(*args, **kwargs):\n",
    "            raise NotImplementedError(\"train_bpe not imported. Run nbdev_export for 02_bpe_training.ipynb\")\n",
    "        def save_tokenizer_components(*args, **kwargs):\n",
    "            raise NotImplementedError(\"save_tokenizer_components not imported. Run nbdev_export for 00_utils.ipynb\")\n",
    "        def load_tokenizer_components(*args, **kwargs):\n",
    "            raise NotImplementedError(\"load_tokenizer_components not imported. Run nbdev_export for 00_utils.ipynb\")\n",
    "        class Tokenizer:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                raise NotImplementedError(\"Tokenizer not imported. Run nbdev_export for 03_bpe_tokenizer_class.ipynb\")\n",
    "            @classmethod\n",
    "            def from_files(cls, *args, **kwargs):\n",
    "                raise NotImplementedError(\"Tokenizer.from_files not imported.\")\n",
    "            def encode(self, *args, **kwargs):\n",
    "                raise NotImplementedError(\"Tokenizer.encode not imported.\")\n",
    "            def decode(self, *args, **kwargs):\n",
    "                raise NotImplementedError(\"Tokenizer.decode not imported.\")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quick Encode/Decode Verification\n",
    "\n",
    "This section provides a quick way to verify the `encode` and `decode` functionality of the `Tokenizer` using a very small, self-contained corpus. This is useful for rapid checks without processing large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_quick_encode_decode_verification():\n",
    "    \"\"\"Trains a BPE tokenizer on a small sample text and verifies encode/decode roundtrip.\"\"\"\n",
    "    print(\"--- Starting Quick Encode/Decode Verification ---\")\n",
    "    \n",
    "    # 1. Define a small, multi-line string corpus\n",
    "    sample_corpus = (\n",
    "        \"hello world.\\n\"\n",
    "        \"this is a test.\\n\"\n",
    "        \"hello again, world!\\n\"\n",
    "        \"special tokens like <|endoftext|> should be handled.\\n\"\n",
    "        \"another line for more content and merges. aabbcc aabb dd.\"\n",
    "    )\n",
    "    special_tokens_for_quick_test = [\"<|endoftext|>\"]\n",
    "    quick_vocab_size = 256 + 15 # Base bytes + a few merges\n",
    "\n",
    "    # Create a temporary file for the corpus\n",
    "    temp_corpus_file = None\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as tmp_f:\n",
    "            tmp_f.write(sample_corpus)\n",
    "            temp_corpus_file = tmp_f.name\n",
    "        print(f\"Small corpus written to temporary file: {temp_corpus_file}\")\n",
    "\n",
    "        # 2. Call train_bpe on this corpus\n",
    "        print(f\"Training BPE on small corpus with vocab_size={quick_vocab_size}...\")\n",
    "        quick_vocab, quick_merges = train_bpe(\n",
    "            input_path=temp_corpus_file,\n",
    "            vocab_size=quick_vocab_size,\n",
    "            special_tokens=special_tokens_for_quick_test\n",
    "        )\n",
    "        print(f\"Training complete. Vocab size: {len(quick_vocab)}, Merges: {len(quick_merges)}\")\n",
    "\n",
    "        # 3. Instantiate the Tokenizer\n",
    "        quick_tokenizer = Tokenizer(quick_vocab, quick_merges, special_tokens_for_quick_test)\n",
    "        print(\"Tokenizer instantiated.\")\n",
    "\n",
    "        # 4. Perform encode/decode roundtrip tests\n",
    "        test_strings = [\n",
    "            \"hello world.\",\n",
    "            \"this is a test.\",\n",
    "            \"hello again, world!\",\n",
    "            \"<|endoftext|> handled?\",\n",
    "            sample_corpus, # Test with the full training corpus as well\n",
    "            \"new string not in training data aabbcc\",\n",
    "            \"â‚¬uro symbol and some more text.\",\n",
    "            \"\"\n",
    "        ]\n",
    "\n",
    "        all_passed = True\n",
    "        for i, text_to_test in enumerate(test_strings):\n",
    "            print(f\"\\nTest string {i+1}: '{text_to_test[:100].replace('\\n', '\\\\n')}{'...' if len(text_to_test) > 100 else ''}'\")\n",
    "            try:\n",
    "                encoded_ids = quick_tokenizer.encode(text_to_test)\n",
    "                print(f\"  Encoded IDs ({len(encoded_ids)}): {encoded_ids[:20]}{'...' if len(encoded_ids) > 20 else ''}\")\n",
    "                decoded_text = quick_tokenizer.decode(encoded_ids)\n",
    "                print(f\"  Decoded text: '{decoded_text[:100].replace('\\n', '\\\\n')}{'...' if len(decoded_text) > 100 else ''}'\")\n",
    "                \n",
    "                if decoded_text == text_to_test:\n",
    "                    print(f\"  PASSED: Roundtrip successful.\")\n",
    "                else:\n",
    "                    print(f\"  FAILED: Decoded text does not match original.\")\n",
    "                    print(f\"    Original: '{text_to_test!r}'\")\n",
    "                    print(f\"    Decoded:  '{decoded_text!r}'\")\n",
    "                    all_passed = False\n",
    "            except Exception as e:\n",
    "                print(f\"  ERRORED during encode/decode: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                all_passed = False\n",
    "        \n",
    "        if all_passed:\n",
    "            print(\"\\nSUCCESS: All quick encode/decode verification tests passed.\")\n",
    "        else:\n",
    "            print(\"\\nFAILURE: Some quick encode/decode verification tests failed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during quick verification: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if temp_corpus_file and os.path.exists(temp_corpus_file):\n",
    "            os.remove(temp_corpus_file)\n",
    "            print(f\"Cleaned up temporary corpus file: {temp_corpus_file}\")\n",
    "            \n",
    "    print(\"--- Quick Encode/Decode Verification Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Acquisition and Preparation\n",
    "\n",
    "This section implements functions to download TinyStories and OpenWebText datasets using the `datasets` library from Hugging Face and prepares them into raw text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path for raw data and tokenizers\n",
    "DATA_RAW_PATH = Path(project_root / \"data/raw\") # project_root is defined in the first cell\n",
    "DATA_TOKENIZERS_PATH = Path(project_root / \"data/tokenizers\")\n",
    "DATA_TOKENIZED_DATASETS_PATH = Path(project_root / \"data/tokenized_datasets\") \n",
    "\n",
    "# Special token delimiter to be used between documents\n",
    "SPECIAL_TOKEN_DELIMITER = \"<|endoftext|>\"\n",
    "\n",
    "TINYSTORIES_TRAIN_FILE = DATA_RAW_PATH / \"tinystories_train.txt\"\n",
    "TINYSTORIES_DEV_FILE = DATA_RAW_PATH / \"tinystories_dev.txt\"\n",
    "OPENWEBTEXT_TRAIN_FILE = DATA_RAW_PATH / \"openwebtext_train.txt\"\n",
    "OPENWEBTEXT_DEV_FILE = DATA_RAW_PATH / \"openwebtext_dev.txt\"\n",
    "\n",
    "TINYSTORIES_VOCAB_FILE = DATA_TOKENIZERS_PATH / \"tinystories_vocab.json\"\n",
    "TINYSTORIES_MERGES_FILE = DATA_TOKENIZERS_PATH / \"tinystories_merges.json\"\n",
    "OPENWEBTEXT_VOCAB_FILE = DATA_TOKENIZERS_PATH / \"openwebtext_vocab.json\"\n",
    "OPENWEBTEXT_MERGES_FILE = DATA_TOKENIZERS_PATH / \"openwebtext_merges.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_data_dir_exists(path: Path) -> None:\n",
    "    \"\"\"Ensures that the directory for the given path exists.\n",
    "\n",
    "    Args:\n",
    "        path (Path): The file path for which the parent directory needs to exist.\n",
    "    \"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _process_and_save_dataset_split(\n",
    "    dataset_split: Iterable[Dict[str, any]], \n",
    "    output_file: Path, \n",
    "    delimiter: str,\n",
    "    text_field: str = \"text\",\n",
    "    limit: Optional[int] = None\n",
    "):\n",
    "    \"\"\"Processes a dataset split, concatenates text documents, and saves to a file.\n",
    "\n",
    "    Args:\n",
    "        dataset_split (Iterable[Dict[str, any]]): The Hugging Face dataset split to process.\n",
    "        output_file (Path): The path to save the concatenated text file.\n",
    "        delimiter (str): The delimiter string to insert between documents.\n",
    "        text_field (str): The name of the field in the dataset dictionary that contains the text.\n",
    "        limit (Optional[int]): If provided, limit processing to this many documents.\n",
    "    \"\"\"\n",
    "    _ensure_data_dir_exists(output_file)\n",
    "    count = 0\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for example in tqdm.tqdm(dataset_split, desc=f\"Processing {output_file.name}\"):\n",
    "            if limit is not None and count >= limit:\n",
    "                break\n",
    "            try:\n",
    "                text_content = example[text_field]\n",
    "                if text_content and isinstance(text_content, str):\n",
    "                    f.write(text_content)\n",
    "                    f.write(delimiter) # Add delimiter after each document\n",
    "                    count += 1\n",
    "                elif not text_content:\n",
    "                    # print(f\"Warning: Empty text content in an example. Skipping.\") # Can be too verbose\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f\"Warning: Text content is not a string ({type(text_content)}). Skipping: {str(text_content)[:100]}...\")\n",
    "            except KeyError:\n",
    "                print(f\"Warning: Text field '{text_field}' not found in an example. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing an example: {e}. Skipping.\")\n",
    "    print(f\"Finished processing. {count} documents written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tinystories_dataset(\n",
    "    output_train_file: Path = TINYSTORIES_TRAIN_FILE,\n",
    "    output_dev_file: Path = TINYSTORIES_DEV_FILE,\n",
    "    delimiter: str = SPECIAL_TOKEN_DELIMITER\n",
    ") -> None:\n",
    "    \"\"\"Downloads and prepares the TinyStories dataset.\n",
    "\n",
    "    Downloads the 'train' and 'validation' splits, concatenates documents \n",
    "    with the specified delimiter, and saves them to text files.\n",
    "\n",
    "    Args:\n",
    "        output_train_file (Path): Path to save the training data.\n",
    "        output_dev_file (Path): Path to save the development/validation data.\n",
    "        delimiter (str): Delimiter to use between documents.\n",
    "    \"\"\"\n",
    "    print(\"Preparing TinyStories dataset...\")\n",
    "    try:\n",
    "        # Download train split\n",
    "        print(\"Loading TinyStories train split...\")\n",
    "        ts_train = datasets.load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
    "        _process_and_save_dataset_split(ts_train, output_train_file, delimiter)\n",
    "\n",
    "        # Download validation split\n",
    "        print(\"Loading TinyStories validation split...\")\n",
    "        ts_dev = datasets.load_dataset(\"roneneldan/TinyStories\", split=\"validation\", streaming=True)\n",
    "        _process_and_save_dataset_split(ts_dev, output_dev_file, delimiter)\n",
    "\n",
    "        print(\"TinyStories dataset preparation complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during TinyStories dataset preparation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_openwebtext_dataset(\n",
    "    output_train_file: Path = OPENWEBTEXT_TRAIN_FILE,\n",
    "    output_dev_file: Path = OPENWEBTEXT_DEV_FILE,\n",
    "    delimiter: str = SPECIAL_TOKEN_DELIMITER,\n",
    "    dev_split_size: int = 5000, # Number of documents for the dev split from train\n",
    "    train_split_limit: Optional[int] = None # Optional limit for the full train split processing\n",
    ") -> None:\n",
    "    \"\"\"Downloads and prepares the OpenWebText dataset.\n",
    "\n",
    "    Downloads the 'train' split, concatenates documents with the delimiter, \n",
    "    and saves it. A smaller dev split is created from the beginning of the train split.\n",
    "\n",
    "    Args:\n",
    "        output_train_file (Path): Path to save the training data.\n",
    "        output_dev_file (Path): Path to save the development data.\n",
    "        delimiter (str): Delimiter to use between documents.\n",
    "        dev_split_size (int): Number of documents from the train set to use for the dev set.\n",
    "        train_split_limit (Optional[int]): If provided, limit the full train split processing to this many documents.\n",
    "    \"\"\"\n",
    "    print(\"Preparing OpenWebText dataset...\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"Creating dev split from OpenWebText (first {dev_split_size} documents)...\")\n",
    "        owt_dev_iterable = datasets.load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True)\n",
    "        _process_and_save_dataset_split(owt_dev_iterable, output_dev_file, delimiter, limit=dev_split_size)\n",
    "\n",
    "        print(\"Processing full OpenWebText train split...\")\n",
    "        # Note: Processing the full OpenWebText can take a very long time and a lot of disk space.\n",
    "        owt_train_full_iterable = datasets.load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True) \n",
    "        _process_and_save_dataset_split(owt_train_full_iterable, output_train_file, delimiter, limit=train_split_limit)\n",
    "\n",
    "        print(\"OpenWebText dataset preparation complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during OpenWebText dataset preparation: {e}\")\n",
    "        print(\"Please ensure you have enough disk space and a stable internet connection.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BPE Training Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_tinystories): BPE Training on TinyStories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_bpe_tinystories_experiment():\n",
    "    \"\"\"Runs the BPE training experiment on the TinyStories dataset.\"\"\"\n",
    "    print(\"--- Starting BPE Training on TinyStories ---\")\n",
    "    \n",
    "    _ensure_data_dir_exists(TINYSTORIES_VOCAB_FILE) # Ensures data/tokenizers directory exists\n",
    "\n",
    "    special_tokens_ts = [SPECIAL_TOKEN_DELIMITER] # Using \"<|endoftext|>\" as special token\n",
    "    vocab_size_ts = 10000\n",
    "\n",
    "    if not TINYSTORIES_TRAIN_FILE.exists():\n",
    "        print(f\"Error: TinyStories training file not found at {TINYSTORIES_TRAIN_FILE}.\")\n",
    "        print(\"Please run the dataset preparation step first.\")\n",
    "        return None, None # Return None if training fails\n",
    "\n",
    "    print(f\"Training BPE on: {TINYSTORIES_TRAIN_FILE}\")\n",
    "    print(f\"Target vocab size: {vocab_size_ts}\")\n",
    "    print(f\"Special tokens: {special_tokens_ts}\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    vocab_ts, merges_ts = None, None\n",
    "    try:\n",
    "        vocab_ts, merges_ts = train_bpe(\n",
    "            input_path=str(TINYSTORIES_TRAIN_FILE),\n",
    "            vocab_size=vocab_size_ts,\n",
    "            special_tokens=special_tokens_ts\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during train_bpe for TinyStories: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None # Return None if training fails\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    training_time_seconds = end_time - start_time\n",
    "\n",
    "    print(f\"BPE training for TinyStories completed in {training_time_seconds:.2f} seconds.\")\n",
    "\n",
    "    # Serialize vocab and merges\n",
    "    print(f\"Serializing vocabulary to: {TINYSTORIES_VOCAB_FILE}\")\n",
    "    print(f\"Serializing merges to: {TINYSTORIES_MERGES_FILE}\")\n",
    "    try:\n",
    "        if vocab_ts is not None and merges_ts is not None:\n",
    "             save_tokenizer_components(vocab_ts, merges_ts, str(TINYSTORIES_VOCAB_FILE), str(TINYSTORIES_MERGES_FILE))\n",
    "             print(\"Serialization complete.\")\n",
    "        else:\n",
    "            print(\"Serialization skipped as vocab_ts or merges_ts is None.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during serialization: {e}\")\n",
    "        return vocab_ts, merges_ts # Return what we have even if serialization fails\n",
    "\n",
    "    # Report: Longest token\n",
    "    if vocab_ts:\n",
    "        max_len_token_bytes = max(vocab_ts.values(), key=len, default=b'')\n",
    "        max_len = len(max_len_token_bytes)\n",
    "        try:\n",
    "            max_len_token_str = max_len_token_bytes.decode('utf-8', 'replace')\n",
    "        except Exception as e:\n",
    "            max_len_token_str = f\"(Error decoding: {e}) {max_len_token_bytes!r}\"\n",
    "        print(f\"Longest token found in TinyStories vocab: Length={max_len} bytes, Content (decoded): '{max_len_token_str}'\")\n",
    "        print(\"  Consider if this token represents a common, repetitive sequence in TinyStories.\")\n",
    "    else:\n",
    "        print(\"TinyStories vocabulary is empty or None, cannot determine longest token.\")\n",
    "\n",
    "    print(f\"TinyStories Training Time: {training_time_seconds:.2f} seconds\")\n",
    "    print(\"Peak Memory Estimation: Requires manual observation or a memory profiler.\")\n",
    "    print(\"Profiling `train_bpe` (Bottlenecks): Best done on a subset of data (e.g., TinyStories dev set). Use cProfile or Scalene.\")\n",
    "    print(\"--- TinyStories BPE Training Experiment Finished ---\")\n",
    "    return vocab_ts, merges_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_expts_owt): BPE Training on OpenWebText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_bpe_openwebtext_experiment():\n",
    "    \"\"\"Runs the BPE training experiment on the OpenWebText dataset.\"\"\"\n",
    "    print(\"--- Starting BPE Training on OpenWebText ---\")\n",
    "\n",
    "    _ensure_data_dir_exists(OPENWEBTEXT_VOCAB_FILE) # Ensures data/tokenizers directory exists\n",
    "\n",
    "    special_tokens_owt = [SPECIAL_TOKEN_DELIMITER] \n",
    "    vocab_size_owt = 32000\n",
    "\n",
    "    if not OPENWEBTEXT_TRAIN_FILE.exists():\n",
    "        print(f\"Error: OpenWebText training file not found at {OPENWEBTEXT_TRAIN_FILE}.\")\n",
    "        print(\"Please run the dataset preparation step first.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Training BPE on: {OPENWEBTEXT_TRAIN_FILE}\")\n",
    "    print(f\"Target vocab size: {vocab_size_owt}\")\n",
    "    print(f\"Special tokens: {special_tokens_owt}\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    vocab_owt, merges_owt = None, None\n",
    "    try:\n",
    "        vocab_owt, merges_owt = train_bpe(\n",
    "            input_path=str(OPENWEBTEXT_TRAIN_FILE),\n",
    "            vocab_size=vocab_size_owt,\n",
    "            special_tokens=special_tokens_owt\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during train_bpe for OpenWebText: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    training_time_seconds = end_time - start_time\n",
    "\n",
    "    print(f\"BPE training for OpenWebText completed in {training_time_seconds:.2f} seconds.\")\n",
    "\n",
    "    print(f\"Serializing vocabulary to: {OPENWEBTEXT_VOCAB_FILE}\")\n",
    "    print(f\"Serializing merges to: {OPENWEBTEXT_MERGES_FILE}\")\n",
    "    try:\n",
    "        if vocab_owt is not None and merges_owt is not None:\n",
    "            save_tokenizer_components(vocab_owt, merges_owt, str(OPENWEBTEXT_VOCAB_FILE), str(OPENWEBTEXT_MERGES_FILE))\n",
    "            print(\"Serialization complete.\")\n",
    "        else:\n",
    "            print(\"Serialization skipped as vocab_owt or merges_owt is None.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during serialization for OpenWebText: {e}\")\n",
    "        return vocab_owt, merges_owt # Return what we have\n",
    "\n",
    "    if vocab_owt:\n",
    "        max_len_token_bytes_owt = max(vocab_owt.values(), key=len, default=b'')\n",
    "        max_len_owt = len(max_len_token_bytes_owt)\n",
    "        try:\n",
    "            max_len_token_str_owt = max_len_token_bytes_owt.decode('utf-8', 'replace')\n",
    "        except Exception as e:\n",
    "            max_len_token_str_owt = f\"(Error decoding: {e}) {max_len_token_bytes_owt!r}\"\n",
    "        print(f\"Longest token found in OpenWebText vocab: Length={max_len_owt} bytes, Content (decoded): '{max_len_token_str_owt}'\")\n",
    "        print(\"  Analyze if this token makes sense for OpenWebText (e.g., common HTML/markup, repetitive sequences).\")\n",
    "    else:\n",
    "        print(\"OpenWebText vocabulary is empty or None, cannot determine longest token.\")\n",
    "\n",
    "    print(f\"OpenWebText Training Time: {training_time_seconds:.2f} seconds\")\n",
    "    print(\"--- OpenWebText BPE Training Experiment Finished ---\")\n",
    "    return vocab_owt, merges_owt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare/Contrast Tokenizers\n",
    "\n",
    "**(Qualitative comparison after both tokenizers are trained)**\n",
    "\n",
    "1.  **Common Long Tokens:** \n",
    "    *   *TinyStories:* (Fill in after observing results - likely story-related phrases, character names, simple sentence structures)\n",
    "    *   *OpenWebText:* (Fill in after observing results - likely common web phrases, HTML tags, URLs, programming language keywords if code is present)\n",
    "2.  **Differences in Types of Tokens Learned:**\n",
    "    *   *TinyStories:* Expected to learn tokens corresponding to simpler words, common story elements (e.g., \"Once upon a time\", character names like \"Lily\", \"Tom\"), and repetitive sentence structures found in children's stories.\n",
    "    *   *OpenWebText:* Expected to learn a more diverse set of tokens, including more complex vocabulary, technical terms, potentially HTML/CSS/JavaScript snippets, common URL components, and a wider range of linguistic styles reflective of general web content.\n",
    "3.  **Vocabulary Composition (General Feel):**\n",
    "    *   *TinyStories:* Vocabulary might be smaller naturally if only simple words are present, but BPE will still try to find common subwords. The resulting tokens might be more easily interpretable as whole words or very common word parts relevant to narrative fiction for children.\n",
    "    *   *OpenWebText:* Vocabulary will be more varied. Tokens might represent a broader range of subwords, including those from technical jargon, diverse proper nouns, and possibly artifacts of web scraping (e.g., parts of markup). The distribution of token lengths might also be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizer Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (tokenizer_experiments): (a) Compression Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_compression_ratio_experiment():\n",
    "    \"\"\"Runs the compression ratio experiment for TinyStories and OpenWebText tokenizers.\"\"\"\n",
    "    print(\"--- Starting Compression Ratio Experiment ---\")\n",
    "\n",
    "    special_tokens_list = [SPECIAL_TOKEN_DELIMITER]\n",
    "    num_samples = 10\n",
    "\n",
    "    # Load Tokenizers\n",
    "    tokenizer_ts, tokenizer_owt = None, None\n",
    "    try:\n",
    "        if not (TINYSTORIES_VOCAB_FILE.exists() and TINYSTORIES_MERGES_FILE.exists()):\n",
    "            print(f\"TinyStories tokenizer files not found. Skipping TS part. Searched: {TINYSTORIES_VOCAB_FILE}, {TINYSTORIES_MERGES_FILE}\")\n",
    "        else:\n",
    "            tokenizer_ts = Tokenizer.from_files(str(TINYSTORIES_VOCAB_FILE), str(TINYSTORIES_MERGES_FILE), special_tokens_list)\n",
    "            print(\"TinyStories Tokenizer loaded.\")\n",
    "\n",
    "        if not (OPENWEBTEXT_VOCAB_FILE.exists() and OPENWEBTEXT_MERGES_FILE.exists()):\n",
    "            print(f\"OpenWebText tokenizer files not found. Skipping OWT part. Searched: {OPENWEBTEXT_VOCAB_FILE}, {OPENWEBTEXT_MERGES_FILE}\")\n",
    "        else:\n",
    "            tokenizer_owt = Tokenizer.from_files(str(OPENWEBTEXT_VOCAB_FILE), str(OPENWEBTEXT_MERGES_FILE), special_tokens_list)\n",
    "            print(\"OpenWebText Tokenizer loaded.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading tokenizers: {e}. Ensure training experiments (G.2, G.3) were run and files exist.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading tokenizers: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # --- TinyStories Compression ---\n",
    "    if tokenizer_ts:\n",
    "        print(f\"\\n--- TinyStories Compression (using tokenizer_ts) on {num_samples} samples ---\")\n",
    "        try:\n",
    "            # Using 'validation' split for TinyStories samples as it's smaller and representative for this test\n",
    "            ts_samples_iter = datasets.load_dataset(\"roneneldan/TinyStories\", split=\"validation\", streaming=True).take(num_samples)\n",
    "            ts_samples = list(ts_samples_iter) # Materialize the samples\n",
    "            if not ts_samples:\n",
    "                print(\"No samples loaded for TinyStories. Skipping compression calculation.\")\n",
    "            else:\n",
    "                total_input_bytes_ts = sum(len(doc['text'].encode('utf-8')) for doc in ts_samples if doc.get('text'))\n",
    "                total_ts_tokens = sum(len(tokenizer_ts.encode(doc['text'])) for doc in ts_samples if doc.get('text'))\n",
    "                \n",
    "                if total_ts_tokens > 0:\n",
    "                    compression_ratio_ts = total_input_bytes_ts / total_ts_tokens\n",
    "                    print(f\"Total Input Bytes (TinyStories): {total_input_bytes_ts}\")\n",
    "                    print(f\"Total Tokens (TinyStories with tokenizer_ts): {total_ts_tokens}\")\n",
    "                    print(f\"Compression Ratio (TinyStories with tokenizer_ts): {compression_ratio_ts:.2f} bytes/token\")\n",
    "                else:\n",
    "                    print(\"No tokens generated for TinyStories samples. Cannot calculate compression ratio.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during TinyStories compression calculation: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "\n",
    "    # --- OpenWebText Compression ---\n",
    "    if tokenizer_owt:\n",
    "        print(f\"\\n--- OpenWebText Compression (using tokenizer_owt) on {num_samples} samples ---\")\n",
    "        try:\n",
    "            # Using 'train' split for OWT samples, as it's the main source\n",
    "            owt_samples_iter = datasets.load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True).take(num_samples)\n",
    "            owt_samples = list(owt_samples_iter) # Materialize the samples\n",
    "            if not owt_samples:\n",
    "                print(\"No samples loaded for OpenWebText. Skipping compression calculation.\")\n",
    "            else:\n",
    "                total_input_bytes_owt = sum(len(doc['text'].encode('utf-8')) for doc in owt_samples if doc.get('text'))\n",
    "                total_owt_tokens = sum(len(tokenizer_owt.encode(doc['text'])) for doc in owt_samples if doc.get('text'))\n",
    "\n",
    "                if total_owt_tokens > 0:\n",
    "                    compression_ratio_owt = total_input_bytes_owt / total_owt_tokens\n",
    "                    print(f\"Total Input Bytes (OpenWebText): {total_input_bytes_owt}\")\n",
    "                    print(f\"Total Tokens (OpenWebText with tokenizer_owt): {total_owt_tokens}\")\n",
    "                    print(f\"Compression Ratio (OpenWebText with tokenizer_owt): {compression_ratio_owt:.2f} bytes/token\")\n",
    "                else:\n",
    "                    print(\"No tokens generated for OpenWebText samples. Cannot calculate compression ratio.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during OpenWebText compression calculation: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "\n",
    "    print(\"--- Compression Ratio Experiment Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (tokenizer_experiments): (b) Cross-Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_tokenization_experiment():\n",
    "    \"\"\"Runs the cross-tokenization experiment: OWT samples with TinyStories tokenizer.\"\"\"\n",
    "    print(\"--- Starting Cross-Tokenization Experiment ---\")\n",
    "    \n",
    "    special_tokens_list = [SPECIAL_TOKEN_DELIMITER]\n",
    "    num_samples = 10\n",
    "\n",
    "    # Load TinyStories Tokenizer\n",
    "    tokenizer_ts = None\n",
    "    try:\n",
    "        if not (TINYSTORIES_VOCAB_FILE.exists() and TINYSTORIES_MERGES_FILE.exists()):\n",
    "            print(f\"TinyStories tokenizer files not found. Cannot run cross-tokenization. Searched: {TINYSTORIES_VOCAB_FILE}, {TINYSTORIES_MERGES_FILE}\")\n",
    "            return\n",
    "        tokenizer_ts = Tokenizer.from_files(str(TINYSTORIES_VOCAB_FILE), str(TINYSTORIES_MERGES_FILE), special_tokens_list)\n",
    "        print(\"TinyStories Tokenizer loaded for cross-tokenization.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading TinyStories tokenizer: {e}.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading TinyStories tokenizer: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Get OpenWebText samples\n",
    "    print(f\"\\n--- OpenWebText Cross-Tokenization (using tokenizer_ts) on {num_samples} samples ---\")\n",
    "    owt_samples = []\n",
    "    try:\n",
    "        owt_samples_iter = datasets.load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True).take(num_samples)\n",
    "        owt_samples = list(owt_samples_iter)\n",
    "        if not owt_samples:\n",
    "            print(\"No samples loaded for OpenWebText. Skipping cross-tokenization calculation.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading OpenWebText samples: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Calculate compression\n",
    "    try:\n",
    "        total_input_bytes_owt_sample = sum(len(doc['text'].encode('utf-8')) for doc in owt_samples if doc.get('text'))\n",
    "        total_tokens_owt_on_ts = sum(len(tokenizer_ts.encode(doc['text'])) for doc in owt_samples if doc.get('text'))\n",
    "\n",
    "        if total_tokens_owt_on_ts > 0:\n",
    "            compression_ratio_owt_on_ts = total_input_bytes_owt_sample / total_tokens_owt_on_ts\n",
    "            print(f\"Total Input Bytes (OpenWebText Samples): {total_input_bytes_owt_sample}\")\n",
    "            print(f\"Total Tokens (OpenWebText with tokenizer_ts): {total_tokens_owt_on_ts}\")\n",
    "            print(f\"Compression Ratio (OpenWebText with tokenizer_ts): {compression_ratio_owt_on_ts:.2f} bytes/token\")\n",
    "        else:\n",
    "            print(\"No tokens generated for OpenWebText samples with TinyStories tokenizer. Cannot calculate compression ratio.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenWebText cross-tokenization calculation: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "\n",
    "    print(\"--- Cross-Tokenization Experiment Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative Description of Cross-Tokenization Differences:\n",
    "\n",
    "*(To be filled in after running the experiment and observing the tokenization results)*\n",
    "\n",
    "When tokenizing OpenWebText data with the TinyStories tokenizer (`tokenizer_ts`), we expect to observe:\n",
    "1.  **Higher Number of Tokens / Lower Compression Ratio:** The `tokenizer_ts` vocabulary is optimized for the simpler language and themes of TinyStories. It will likely lack tokens for more complex words, technical jargon, or web-specific constructs (like HTML tags or parts of URLs) common in OpenWebText. As a result, these out-of-domain terms will be broken down into many smaller, more generic subword units or even individual bytes if they are completely unseen.\n",
    "2.  **Shorter Average Token Length (for OWT-specific terms):** Complex words from OWT that would be single tokens (or few tokens) with `tokenizer_owt` will be represented by sequences of shorter tokens by `tokenizer_ts`.\n",
    "3.  **Increased Unknowns (or sub-optimal segmentation):** While our BPE implementation aims to tokenize everything into byte sequences if necessary, the efficiency of representing OWT will be poor. Words or character sequences common in OWT but rare/absent in TinyStories will not have dedicated merged tokens in `tokenizer_ts`.\n",
    "4.  **Examples:**\n",
    "    *   A technical term like \"hyperparameter\" might be tokenized as `['hyper', 'para', 'meter']` or even `['h', 'y', 'p', 'e', 'r', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r']` by `tokenizer_ts` if these sub-parts are not common in TinyStories, whereas `tokenizer_owt` might have `['hyperparameter']` or `['hyper', 'parameter']`.\n",
    "    *   An HTML tag like `<div>` would likely be `['<', 'd', 'i', 'v', '>']` with `tokenizer_ts`, while `tokenizer_owt` might have learned `['<div>']` or `['<div', '>']` if HTML is common in its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (tokenizer_experiments): (c) Throughput Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_throughput_estimation_experiment():\n",
    "    \"\"\"Estimates the tokenization throughput of tokenizer_owt and time to tokenize the Pile dataset.\"\"\"\n",
    "    print(\"--- Starting Throughput Estimation Experiment ---\")\n",
    "\n",
    "    special_tokens_list = [SPECIAL_TOKEN_DELIMITER]\n",
    "\n",
    "    # Load OpenWebText Tokenizer\n",
    "    tokenizer_owt = None\n",
    "    try:\n",
    "        if not (OPENWEBTEXT_VOCAB_FILE.exists() and OPENWEBTEXT_MERGES_FILE.exists()):\n",
    "            print(f\"OpenWebText tokenizer files not found. Cannot run throughput estimation. Searched: {OPENWEBTEXT_VOCAB_FILE}, {OPENWEBTEXT_MERGES_FILE}\")\n",
    "            return\n",
    "        tokenizer_owt = Tokenizer.from_files(str(OPENWEBTEXT_VOCAB_FILE), str(OPENWEBTEXT_MERGES_FILE), special_tokens_list)\n",
    "        print(\"OpenWebText Tokenizer loaded for throughput estimation.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading OpenWebText tokenizer: {e}.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading OpenWebText tokenizer: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Use OpenWebText dev split as the large text file for testing throughput\n",
    "    # The size of this file will affect the reliability of the estimate.\n",
    "    # A larger file (e.g., 10-100MB) is better. OPENWEBTEXT_DEV_FILE might be suitable.\n",
    "    large_text_file_path = OPENWEBTEXT_DEV_FILE\n",
    "    if not large_text_file_path.exists():\n",
    "        print(f\"Error: Large text file for throughput test not found at {large_text_file_path}.\")\n",
    "        print(\"Please ensure OpenWebText dev split (or another large file) is prepared.\")\n",
    "        # Fallback to TinyStories dev if OWT dev is missing, for basic test flow\n",
    "        if TINYSTORIES_DEV_FILE.exists():\n",
    "            print(f\"Falling back to TinyStories dev file: {TINYSTORIES_DEV_FILE}\")\n",
    "            large_text_file_path = TINYSTORIES_DEV_FILE\n",
    "        else:\n",
    "            print(\"No suitable large text file found. Aborting throughput estimation.\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        print(f\"Reading large text file: {large_text_file_path} (size: {large_text_file_path.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "        with open(large_text_file_path, 'r', encoding='utf-8') as f:\n",
    "            large_text_content = f.read()\n",
    "        \n",
    "        if not large_text_content:\n",
    "            print(\"The large text file is empty. Cannot estimate throughput.\")\n",
    "            return\n",
    "\n",
    "        print(\"Estimating throughput for tokenizer_owt.encode()...\")\n",
    "        start_time = time.perf_counter()\n",
    "        _ = tokenizer_owt.encode(large_text_content) # Perform encoding\n",
    "        duration = time.perf_counter() - start_time\n",
    "\n",
    "        input_bytes = len(large_text_content.encode('utf-8'))\n",
    "        \n",
    "        if duration > 0:\n",
    "            bytes_per_second = input_bytes / duration\n",
    "            print(f\"Input size: {input_bytes / (1024*1024):.2f} MB\")\n",
    "            print(f\"Encoding time: {duration:.2f} seconds\")\n",
    "            print(f\"Throughput (tokenizer_owt.encode): {bytes_per_second / (1024*1024):.2f} MB/second\")\n",
    "\n",
    "            # Estimate time to tokenize Pile (825 GiB)\n",
    "            pile_size_gib = 825\n",
    "            pile_size_bytes = pile_size_gib * (1024**3)\n",
    "            estimated_time_seconds_pile = pile_size_bytes / bytes_per_second\n",
    "            estimated_time_days_pile = estimated_time_seconds_pile / (3600 * 24)\n",
    "            print(f\"Estimated time to tokenize The Pile ({pile_size_gib} GiB): {estimated_time_days_pile:.2f} days\")\n",
    "        else:\n",
    "            print(\"Encoding duration was zero. Cannot calculate throughput (file might be too small or timer resolution issue).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during throughput estimation: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "\n",
    "    print(\"--- Throughput Estimation Experiment Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (tokenizer_experiments): (d) Dataset Tokenization and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset_tokenization_and_storage_experiment():\n",
    "    \"\"\"Tokenizes train/dev splits of TinyStories and OpenWebText and saves them as NumPy arrays.\"\"\"\n",
    "    print(\"--- Starting Dataset Tokenization and Storage Experiment ---\")\n",
    "\n",
    "    special_tokens_list = [SPECIAL_TOKEN_DELIMITER]\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    DATA_TOKENIZED_DATASETS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define output .npy file paths\n",
    "    ts_train_npy = DATA_TOKENIZED_DATASETS_PATH / \"tinystories_train.npy\"\n",
    "    ts_dev_npy = DATA_TOKENIZED_DATASETS_PATH / \"tinystories_dev.npy\"\n",
    "    owt_train_npy = DATA_TOKENIZED_DATASETS_PATH / \"openwebtext_train.npy\"\n",
    "    owt_dev_npy = DATA_TOKENIZED_DATASETS_PATH / \"openwebtext_dev.npy\"\n",
    "\n",
    "    # Load Tokenizers\n",
    "    tokenizer_ts, tokenizer_owt = None, None\n",
    "    try:\n",
    "        if not (TINYSTORIES_VOCAB_FILE.exists() and TINYSTORIES_MERGES_FILE.exists()):\n",
    "            print(f\"TinyStories tokenizer files not found. Skipping TS tokenization. Searched: {TINYSTORIES_VOCAB_FILE}, {TINYSTORIES_MERGES_FILE}\")\n",
    "        else:\n",
    "            tokenizer_ts = Tokenizer.from_files(str(TINYSTORIES_VOCAB_FILE), str(TINYSTORIES_MERGES_FILE), special_tokens_list)\n",
    "            print(\"TinyStories Tokenizer loaded.\")\n",
    "\n",
    "        if not (OPENWEBTEXT_VOCAB_FILE.exists() and OPENWEBTEXT_MERGES_FILE.exists()):\n",
    "            print(f\"OpenWebText tokenizer files not found. Skipping OWT tokenization. Searched: {OPENWEBTEXT_VOCAB_FILE}, {OPENWEBTEXT_MERGES_FILE}\")\n",
    "        else:\n",
    "            tokenizer_owt = Tokenizer.from_files(str(OPENWEBTEXT_VOCAB_FILE), str(OPENWEBTEXT_MERGES_FILE), special_tokens_list)\n",
    "            print(\"OpenWebText Tokenizer loaded.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading tokenizers: {e}. Ensure training experiments were run.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading tokenizers: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    def _tokenize_and_save(input_text_file: Path, output_npy_file: Path, tokenizer: Tokenizer, dataset_name: str):\n",
    "        \"\"\"Helper function to read a text file, tokenize, and save as .npy.\"\"\"\n",
    "        if not tokenizer:\n",
    "            print(f\"Tokenizer for {dataset_name} not loaded. Skipping tokenization of {input_text_file.name}.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"\\nTokenizing {dataset_name} split: {input_text_file.name} -> {output_npy_file.name}\")\n",
    "        if not input_text_file.exists():\n",
    "            print(f\"Error: Input text file {input_text_file} not found. Skipping.\")\n",
    "            return\n",
    "        try:\n",
    "            with open(input_text_file, 'r', encoding='utf-8') as f:\n",
    "                text_content = f.read()\n",
    "            \n",
    "            if not text_content:\n",
    "                print(f\"Warning: Text file {input_text_file} is empty. Saving empty array.\")\n",
    "                token_ids_array = np.array([], dtype=np.uint16)\n",
    "            else:\n",
    "                print(f\"Encoding {input_text_file.name}...\")\n",
    "                start_time = time.perf_counter()\n",
    "                token_ids = tokenizer.encode(text_content)\n",
    "                duration = time.perf_counter() - start_time\n",
    "                print(f\"Encoded {len(token_ids)} tokens in {duration:.2f} seconds.\")\n",
    "                token_ids_array = np.array(token_ids, dtype=np.uint16)\n",
    "            \n",
    "            np.save(str(output_npy_file), token_ids_array)\n",
    "            print(f\"Successfully saved tokenized data to {output_npy_file}\")\n",
    "            print(f\"  Shape: {token_ids_array.shape}, Dtype: {token_ids_array.dtype}, Size: {token_ids_array.nbytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error tokenizing or saving {input_text_file.name}: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "\n",
    "    # Tokenize TinyStories splits\n",
    "    if tokenizer_ts:\n",
    "        _tokenize_and_save(TINYSTORIES_TRAIN_FILE, ts_train_npy, tokenizer_ts, \"TinyStories Train\")\n",
    "        _tokenize_and_save(TINYSTORIES_DEV_FILE, ts_dev_npy, tokenizer_ts, \"TinyStories Dev\")\n",
    "    else:\n",
    "        print(\"TinyStories tokenizer not available, skipping its dataset tokenization.\")\n",
    "\n",
    "    # Tokenize OpenWebText splits\n",
    "    if tokenizer_owt:\n",
    "        _tokenize_and_save(OPENWEBTEXT_TRAIN_FILE, owt_train_npy, tokenizer_owt, \"OpenWebText Train\")\n",
    "        _tokenize_and_save(OPENWEBTEXT_DEV_FILE, owt_dev_npy, tokenizer_owt, \"OpenWebText Dev\")\n",
    "    else:\n",
    "        print(\"OpenWebText tokenizer not available, skipping its dataset tokenization.\")\n",
    "\n",
    "    print(\"--- Dataset Tokenization and Storage Experiment Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Justification for `np.uint16`\n",
    "\n",
    "`np.uint16` is an appropriate data type for storing the token IDs for the following reasons:\n",
    "\n",
    "1.  **Range Coverage:** `np.uint16` is an unsigned 16-bit integer, meaning it can represent values from 0 to 2<sup>16</sup> - 1, which is 0 to 65,535.\n",
    "    *   For the TinyStories tokenizer, the target `vocab_size` is 10,000.\n",
    "    *   For the OpenWebText tokenizer, the target `vocab_size` is 32,000.\n",
    "    Both 10,000 and 32,000 fall comfortably within the 0-65,535 range. This ensures that all possible token IDs generated by these tokenizers can be stored without overflow or data loss.\n",
    "\n",
    "2.  **Memory Efficiency:** Using `np.uint16` (2 bytes per token ID) is more memory-efficient than using larger integer types like `np.int32` (4 bytes) or `np.int64` (8 bytes) when the maximum ID does not require the larger range. For large tokenized datasets, this difference in memory usage can be substantial.\n",
    "    *   If we used `np.int32`, we would be using twice the necessary memory.\n",
    "\n",
    "3.  **Non-Negativity:** Token IDs are typically non-negative integers starting from 0, so an unsigned integer type (`uint`) is suitable.\n",
    "\n",
    "Therefore, `np.uint16` provides a good balance between correctly representing all possible token IDs and minimizing memory footprint for the specified vocabulary sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage and Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories '/Users/abhisheksharma/Desktop/src/github/projects/llm_scratch/lm/data/raw',\n",
      "'/Users/abhisheksharma/Desktop/src/github/projects/llm_scratch/lm/data/tokenizers',\n",
      "and '/Users/abhisheksharma/Desktop/src/github/projects/llm_scratch/lm/data/tokenized_datasets' ensured.\n",
      "\n",
      "--- Running Quick Encode/Decode Verification (Section 0) ---\n",
      "--- Starting Quick Encode/Decode Verification ---\n",
      "Small corpus written to temporary file: /var/folders/1_/__n8_ny14_g0tlyfd57xj1rw0000gn/T/tmp9epck826\n",
      "Training BPE on small corpus with vocab_size=271...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Vocab size: 271, Merges: 14\n",
      "Tokenizer instantiated.\n",
      "\n",
      "Test string 1: 'hello world.'\n",
      "  Encoded IDs (12): [105, 102, 109, 109, 112, 33, 120, 112, 115, 109, 101, 47]\n",
      "  Decoded text: 'hello world.'\n",
      "  PASSED: Roundtrip successful.\n",
      "\n",
      "Test string 2: 'this is a test.'\n",
      "  Encoded IDs (15): [117, 105, 106, 116, 33, 106, 116, 33, 98, 33, 117, 102, 116, 117, 47]\n",
      "  Decoded text: 'this is a test.'\n",
      "  PASSED: Roundtrip successful.\n",
      "\n",
      "Test string 3: 'hello again, world!'\n",
      "  Encoded IDs (19): [105, 102, 109, 109, 112, 33, 98, 104, 98, 106, 111, 45, 33, 120, 112, 115, 109, 101, 34]\n",
      "  Decoded text: 'hello again, world!'\n",
      "  PASSED: Roundtrip successful.\n",
      "\n",
      "Test string 4: '<|endoftext|> handled?'\n",
      "  Encoded IDs (10): [0, 33, 105, 98, 111, 101, 109, 102, 101, 64]\n",
      "  Decoded text: '<|endoftext|> handled?'\n",
      "  PASSED: Roundtrip successful.\n",
      "\n",
      "Test string 5: 'hello world.\\nthis is a test.\\nhello again, world!\\nspecial tokens like <|endoftext|> should be handled...'\n",
      "  Encoded IDs (147): [105, 102, 109, 109, 112, 33, 120, 112, 115, 109, 101, 47, 11, 117, 105, 106, 116, 33, 106, 116]...\n",
      "  Decoded text: 'hello world.\\nthis is a test.\\nhello again, world!\\nspecial tokens like <|endoftext|> should be handled...'\n",
      "  PASSED: Roundtrip successful.\n",
      "\n",
      "Test string 6: 'new string not in training data aabbcc'\n",
      "  Encoded IDs (38): [111, 102, 120, 33, 116, 117, 115, 106, 111, 104, 33, 111, 112, 117, 33, 106, 111, 33, 117, 115]...\n",
      "  Decoded text: 'new string not in training data aabbcc'\n",
      "  PASSED: Roundtrip successful.\n",
      "\n",
      "Test string 7: 'â‚¬uro symbol and some more text.'\n",
      "  Encoded IDs (33): [227, 131, 173, 118, 115, 112, 33, 116, 122, 110, 99, 112, 109, 33, 98, 111, 101, 33, 116, 112]...\n",
      "  Decoded text: 'â‚¬uro symbol and some more text.'\n",
      "  PASSED: Roundtrip successful.\n",
      "\n",
      "Test string 8: ''\n",
      "  Encoded IDs (0): []\n",
      "  Decoded text: ''\n",
      "  PASSED: Roundtrip successful.\n",
      "\n",
      "SUCCESS: All quick encode/decode verification tests passed.\n",
      "Cleaned up temporary corpus file: /var/folders/1_/__n8_ny14_g0tlyfd57xj1rw0000gn/T/tmp9epck826\n",
      "--- Quick Encode/Decode Verification Finished ---\n",
      "\n",
      "Experiment script finished. Uncomment calls in __main__ to run specific parts.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Ensure the base data directories exist\n",
    "    DATA_RAW_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    DATA_TOKENIZERS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    DATA_TOKENIZED_DATASETS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Data directories '{DATA_RAW_PATH.resolve()}',\\n'{DATA_TOKENIZERS_PATH.resolve()}',\\nand '{DATA_TOKENIZED_DATASETS_PATH.resolve()}' ensured.\")\n",
    "\n",
    "    # --- Quick Encode/Decode Verification (NEW) ---\n",
    "    print(\"\\n--- Running Quick Encode/Decode Verification (Section 0) ---\")\n",
    "    run_quick_encode_decode_verification()\n",
    "\n",
    "    # --- Dataset Preparation (Section 1) ---\n",
    "    # print(\"\\n--- Preparing TinyStories (Section 1) ---\")\n",
    "    # prepare_tinystories_dataset()\n",
    "    # print(\"\\n--- Preparing OpenWebText (Section 1) ---\")\n",
    "    # # Reduce dev_split_size for quicker testing if needed, e.g., dev_split_size=100\n",
    "    # # Also consider limiting the main train split for OWT during initial testing due to its size.\n",
    "    # # prepare_openwebtext_dataset(dev_split_size=100, train_split_limit=1000) # Example: 100 for dev, 1000 for train limit\n",
    "    # prepare_openwebtext_dataset() # For full OWT prep\n",
    "\n",
    "    # --- BPE Training Experiment for TinyStories (Section 2) ---\n",
    "    # print(\"\\n--- Running TinyStories BPE Training Experiment (Section 2) ---\")\n",
    "    # vocab_ts, merges_ts = run_train_bpe_tinystories_experiment()\n",
    "\n",
    "    # --- BPE Training Experiment for OpenWebText (Section 2) ---\n",
    "    # print(\"\\n--- Running OpenWebText BPE Training Experiment (Section 2) ---\")\n",
    "    # vocab_owt, merges_owt = run_train_bpe_openwebtext_experiment()\n",
    "\n",
    "    # --- Tokenizer Experiments (Section 3) ---\n",
    "    # Part (a) Compression Ratio\n",
    "    # print(\"\\n--- Running Compression Ratio Experiment (Section 3a) ---\")\n",
    "    # run_compression_ratio_experiment()\n",
    "    \n",
    "    # Part (b) Cross-Tokenization\n",
    "    # print(\"\\n--- Running Cross-Tokenization Experiment (Section 3b) ---\")\n",
    "    # run_cross_tokenization_experiment()\n",
    "\n",
    "    # Part (c) Throughput Estimation\n",
    "    # print(\"\\n--- Running Throughput Estimation Experiment (Section 3c) ---\")\n",
    "    # run_throughput_estimation_experiment()\n",
    "\n",
    "    # Part (d) Dataset Tokenization and Storage\n",
    "    # print(\"\\n--- Running Dataset Tokenization and Storage Experiment (Section 3d) ---\")\n",
    "    # run_dataset_tokenization_and_storage_experiment()\n",
    "\n",
    "    print(\"\\nExperiment script finished. Uncomment calls in __main__ to run specific parts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
