{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Utility functions for the BPE Tokenizer project, primarily for serialization/deserialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import base64\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def save_tokenizer_components(\n",
    "    vocab: Dict[int, bytes], \n",
    "    merges: List[Tuple[bytes, bytes]], \n",
    "    vocab_filepath: str, \n",
    "    merges_filepath: str\n",
    "):\n",
    "    \"\"\"Saves tokenizer vocab and merges to JSON files with Base64 encoding for bytes.\n",
    "\n",
    "    Args:\n",
    "        vocab (Dict[int, bytes]): Vocabulary mapping token IDs to byte sequences.\n",
    "        merges (List[Tuple[bytes, bytes]]): List of merge rules (pairs of byte sequences).\n",
    "        vocab_filepath (str): Path to save the vocabulary JSON file.\n",
    "        merges_filepath (str): Path to save the merges JSON file.\n",
    "    \"\"\"\n",
    "    # Convert vocab: keys to str, bytes values to base64 encoded ascii strings\n",
    "    vocab_to_save = {str(k): base64.b64encode(v).decode('ascii') for k, v in vocab.items()}\n",
    "    with open(vocab_filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab_to_save, f, indent=2)\n",
    "\n",
    "    # Convert merges: each byte sequence in pairs to base64 encoded ascii strings\n",
    "    merges_to_save = [\n",
    "        (base64.b64encode(p1).decode('ascii'), base64.b64encode(p2).decode('ascii')) \n",
    "        for p1, p2 in merges\n",
    "    ]\n",
    "    with open(merges_filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merges_to_save, f, indent=2)\n",
    "\n",
    "#| export\n",
    "def load_tokenizer_components(\n",
    "    vocab_filepath: str, \n",
    "    merges_filepath: str\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"Loads tokenizer vocab and merges from JSON files, decoding Base64 for bytes.\n",
    "\n",
    "    Args:\n",
    "        vocab_filepath (str): Path to the vocabulary JSON file.\n",
    "        merges_filepath (str): Path to the merges JSON file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]: Loaded vocabulary and merges.\n",
    "    \"\"\"\n",
    "    # Load vocab: convert str keys to int, base64 encoded ascii strings to bytes\n",
    "    with open(vocab_filepath, 'r', encoding='utf-8') as f:\n",
    "        vocab_json = json.load(f)\n",
    "    loaded_vocab = {int(k): base64.b64decode(v_str) for k, v_str in vocab_json.items()}\n",
    "\n",
    "    # Load merges: convert base64 encoded ascii strings in pairs back to bytes\n",
    "    with open(merges_filepath, 'r', encoding='utf-8') as f:\n",
    "        merges_json = json.load(f)\n",
    "    loaded_merges = [\n",
    "        (base64.b64decode(p1_str), base64.b64decode(p2_str)) \n",
    "        for p1_str, p2_str in merges_json\n",
    "    ]\n",
    "    \n",
    "    return loaded_vocab, loaded_merges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}