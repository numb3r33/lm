{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Training\n",
    "\n",
    "> Implementation of the `train_bpe` function for Byte-Pair Encoding tokenizer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp bpe_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import regex as re\n",
    "import json\n",
    "import base64\n",
    "from collections import Counter, defaultdict\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from typing import List, Dict, Tuple, Any, Pattern, Set\n",
    "import os\n",
    "import tempfile # For example usage/test block\n",
    "import time # For example usage/test block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# GPT-2 pre-tokenization regex from the technical specification\n",
    "PAT_REGEX: Pattern = re.compile(r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _process_segment(segment_text: str, pat_regex: Pattern) -> List[List[int]]:\n",
    "    \"\"\"Internal helper function to pre-tokenize a text segment and convert to UTF-8 byte lists.\n",
    "\n",
    "    Args:\n",
    "        segment_text: A string segment of the corpus.\n",
    "        pat_regex: The pre-compiled regex pattern for pre-tokenization.\n",
    "\n",
    "    Returns:\n",
    "        A list where each inner list contains the UTF-8 byte values (integers 0-255)\n",
    "        for a pre-token found in the segment_text.\n",
    "    \"\"\"\n",
    "    pre_tokens_as_int_lists: List[List[int]] = []\n",
    "    if not segment_text: # Handle empty segments gracefully\n",
    "        return []\n",
    "    for match in pat_regex.finditer(segment_text):\n",
    "        pre_token_str = match.group(0)\n",
    "        pre_tokens_as_int_lists.append(list(pre_token_str.encode('utf-8')))\n",
    "    return pre_tokens_as_int_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def _replace_pair_in_sequence(\n",
    "    sequence: Tuple[int, ...],\n",
    "    pair_to_replace: Tuple[int, int],\n",
    "    new_id: int\n",
    ") -> Tuple[int, ...]:\n",
    "    \"\"\"Replaces all occurrences of a pair of token IDs with a new token ID in a sequence.\n",
    "\n",
    "    Args:\n",
    "        sequence: The tuple of token IDs representing a word/token.\n",
    "        pair_to_replace: A tuple of two token IDs (e.g., (id1, id2)) to be replaced.\n",
    "        new_id: The new token ID that will replace the pair.\n",
    "\n",
    "    Returns:\n",
    "        A new tuple of token IDs with all occurrences of the pair replaced.\n",
    "    \"\"\"\n",
    "    new_sequence_list: List[int] = []\n",
    "    i = 0\n",
    "    while i < len(sequence):\n",
    "        # Check if the current position and the next form the pair to replace\n",
    "        if i + 1 < len(sequence) and \\\n",
    "           sequence[i] == pair_to_replace[0] and \\\n",
    "           sequence[i+1] == pair_to_replace[1]:\n",
    "            new_sequence_list.append(new_id)\n",
    "            i += 2 # Move past the pair\n",
    "        else:\n",
    "            new_sequence_list.append(sequence[i])\n",
    "            i += 1 # Move to the next token\n",
    "    return tuple(new_sequence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_bpe(\n",
    "    input_path: str,         # Path to the raw text file\n",
    "    vocab_size: int,         # Desired final vocabulary size\n",
    "    special_tokens: List[str] # List of special token strings\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"Trains a Byte-Pair Encoding (BPE) tokenizer from a raw text file.\n",
    "\n",
    "    Args:\n",
    "        input_path: Path to the input text file (UTF-8 encoded).\n",
    "        vocab_size: The target size of the vocabulary to be learned.\n",
    "        special_tokens: A list of strings representing special tokens (e.g., '<|endoftext|>').\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - vocab (Dict[int, bytes]): The learned vocabulary mapping token IDs (int) to byte sequences (bytes).\n",
    "        - merges (List[Tuple[bytes, bytes]]): The list of learned BPE merge rules, where each rule\n",
    "          is a tuple of two byte sequences that are merged.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If vocab_size is too small to accommodate base bytes and special tokens.\n",
    "        FileNotFoundError: If input_path does not exist.\n",
    "    \"\"\"\n",
    "    # nbdev: CWD to project root for pathing (this is a comment, nbdev handles it for notebook execution)\n",
    "    \n",
    "    # Calculate the number of unique byte sequences from special tokens\n",
    "    unique_special_token_byte_values: Set[bytes] = set()\n",
    "    if special_tokens:\n",
    "        for st_str in special_tokens:\n",
    "            unique_special_token_byte_values.add(st_str.encode(\"utf-8\"))\n",
    "    \n",
    "    min_required_vocab_size_for_assertion = 256 # Start with base bytes\n",
    "    temp_base_bytes_for_assertion = {bytes([i]) for i in range(256)}\n",
    "    for st_byte_val in unique_special_token_byte_values:\n",
    "        if st_byte_val not in temp_base_bytes_for_assertion:\n",
    "            min_required_vocab_size_for_assertion +=1\n",
    "\n",
    "    assert vocab_size >= min_required_vocab_size_for_assertion, \\\n",
    "        f\"vocab_size ({vocab_size}) must be at least {min_required_vocab_size_for_assertion} to accommodate all unique special tokens and 256 base bytes.\"\n",
    "\n",
    "    vocab: Dict[int, bytes] = {}\n",
    "    current_id: int = 0\n",
    "    merges: List[Tuple[bytes, bytes]] = [] \n",
    "\n",
    "    # 1. Vocabulary Initialization (Step D.2)\n",
    "    _temp_seen_special_bytes: Set[bytes] = set()\n",
    "    temp_special_vocab: Dict[int, bytes] = {} \n",
    "    temp_special_id_counter: int = 0\n",
    "    \n",
    "    if special_tokens:\n",
    "        unique_st_strings = sorted(list(set(special_tokens)))\n",
    "        for st_str in unique_st_strings:\n",
    "            st_bytes = st_str.encode(\"utf-8\")\n",
    "            if st_bytes not in _temp_seen_special_bytes:\n",
    "                temp_special_vocab[temp_special_id_counter] = st_bytes\n",
    "                _temp_seen_special_bytes.add(st_bytes)\n",
    "                temp_special_id_counter += 1\n",
    "    \n",
    "    for _temp_id, byte_val in sorted(temp_special_vocab.items()): \n",
    "        vocab[current_id] = byte_val\n",
    "        current_id += 1\n",
    "        \n",
    "    current_vocab_byte_values = set(vocab.values()) \n",
    "    for i in range(256):\n",
    "        byte_val = bytes([i])\n",
    "        if byte_val not in current_vocab_byte_values:\n",
    "            vocab[current_id] = byte_val\n",
    "            current_id += 1\n",
    "            current_vocab_byte_values.add(byte_val) \n",
    "            \n",
    "    initial_vocab_size: int = len(vocab)\n",
    "    if vocab_size < initial_vocab_size:\n",
    "        print(f\"Warning: Requested vocab_size {vocab_size} is less than the initial vocab size {initial_vocab_size} \\n\"\n",
    "              f\"(derived from special tokens and 256 base bytes). \\n\"\n",
    "              f\"Returning the initial vocabulary of size {initial_vocab_size}. No merges will be performed.\")\n",
    "        return vocab, merges\n",
    "\n",
    "    # 2. Corpus Processing and Parallel Pre-tokenization (Step D.3)\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            corpus_text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at {input_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading input file {input_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "    text_segments: List[str]\n",
    "    if special_tokens:\n",
    "        escaped_special_tokens = [re.escape(st) for st in sorted(list(set(special_tokens)), key=len, reverse=True)]\n",
    "        delimiter_pattern_str = \"|\".join(escaped_special_tokens)\n",
    "        delimiter_pattern = re.compile(delimiter_pattern_str)\n",
    "        text_segments = delimiter_pattern.split(corpus_text)\n",
    "    else:\n",
    "        text_segments = [corpus_text]\n",
    "    \n",
    "    all_pre_tokens_int_lists: List[List[int]] = []\n",
    "    num_processes = 1\n",
    "    if text_segments and any(s.strip() for s in text_segments):\n",
    "        try:\n",
    "            non_empty_segments = [s for s in text_segments if s.strip()] \n",
    "            if non_empty_segments:\n",
    "                num_processes = min(cpu_count(), len(non_empty_segments))\n",
    "                if num_processes == 0: num_processes = 1 \n",
    "                \n",
    "                with Pool(processes=num_processes) as pool:\n",
    "                    map_func = partial(_process_segment, pat_regex=PAT_REGEX)\n",
    "                    try:\n",
    "                        results_list_of_lists = pool.map(map_func, non_empty_segments)\n",
    "                        for sublist in results_list_of_lists:\n",
    "                            all_pre_tokens_int_lists.extend(sublist)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error during parallel pre-tokenization: {e}\")\n",
    "                        pass \n",
    "        except NotImplementedError: \n",
    "            num_processes = 1 \n",
    "            print(\"Warning: cpu_count() not implemented. Defaulting to 1 process for pre-tokenization.\")\n",
    "            if not all_pre_tokens_int_lists: \n",
    "                for segment_text in text_segments:\n",
    "                    if segment_text.strip(): \n",
    "                        all_pre_tokens_int_lists.extend(_process_segment(segment_text, PAT_REGEX))\n",
    "        except Exception as e: \n",
    "            print(f\"An unexpected error occurred with multiprocessing.Pool: {e}. Defaulting to sequential pre-tokenization.\")\n",
    "            num_processes = 1\n",
    "            if not all_pre_tokens_int_lists: \n",
    "                for segment_text in text_segments:\n",
    "                    if segment_text.strip():\n",
    "                        all_pre_tokens_int_lists.extend(_process_segment(segment_text, PAT_REGEX))\n",
    "    \n",
    "    if not all_pre_tokens_int_lists and vocab_size > initial_vocab_size:\n",
    "         print(f\"Warning: No tokens found in corpus '{input_path}' after pre-tokenization. \\n\"\n",
    "               f\"Returning initial vocab of size {initial_vocab_size}. Cannot reach target vocab_size {vocab_size}.\")\n",
    "         return vocab, merges \n",
    "\n",
    "    # 3. Word Counts (Step D.4)\n",
    "    word_counts = Counter(tuple(token_as_int_list) for token_as_int_list in all_pre_tokens_int_lists if token_as_int_list)\n",
    "    \n",
    "    # 4. BPE Merge Loop (Step D.6)\n",
    "    num_merges_needed = vocab_size - len(vocab)\n",
    "    byte_map: Dict[int, bytes] = vocab.copy() \n",
    "\n",
    "    for i in range(num_merges_needed):\n",
    "        if not word_counts:\n",
    "            print(f\"Warning: Word counts became empty. Stopping BPE training early at vocab size {len(vocab)} after {i} merges.\")\n",
    "            break\n",
    "\n",
    "        pair_freqs = Counter()\n",
    "        for token_tuple_ints, count in word_counts.items():\n",
    "            for j in range(len(token_tuple_ints) - 1):\n",
    "                pair = (token_tuple_ints[j], token_tuple_ints[j+1])\n",
    "                pair_freqs[pair] += count\n",
    "        \n",
    "        if not pair_freqs:\n",
    "            print(f\"Warning: No more pairs to merge. Stopping BPE training early at vocab size {len(vocab)} after {i} merges.\")\n",
    "            break\n",
    "\n",
    "        # Find Best Pair\n",
    "        max_freq = max(pair_freqs.values())\n",
    "        candidate_pairs = [p for p, freq in pair_freqs.items() if freq == max_freq]\n",
    "        \n",
    "        best_pair_int_tuple = candidate_pairs[0]\n",
    "        if len(candidate_pairs) > 1:\n",
    "            max_byte_tuple_representation = (byte_map[best_pair_int_tuple[0]], byte_map[best_pair_int_tuple[1]])\n",
    "            for k_idx in range(1, len(candidate_pairs)):\n",
    "                p_int_candidate = candidate_pairs[k_idx]\n",
    "                current_byte_tuple_representation = (byte_map[p_int_candidate[0]], byte_map[p_int_candidate[1]])\n",
    "                if current_byte_tuple_representation > max_byte_tuple_representation:\n",
    "                    max_byte_tuple_representation = current_byte_tuple_representation\n",
    "                    best_pair_int_tuple = p_int_candidate\n",
    "            \n",
    "        # Perform Merge\n",
    "        new_token_id = current_id\n",
    "        \n",
    "        p1_bytes = byte_map[best_pair_int_tuple[0]]\n",
    "        p2_bytes = byte_map[best_pair_int_tuple[1]]\n",
    "        merged_bytes = p1_bytes + p2_bytes\n",
    "\n",
    "        vocab[new_token_id] = merged_bytes\n",
    "        byte_map[new_token_id] = merged_bytes \n",
    "        merges.append((p1_bytes, p2_bytes)) \n",
    "        current_id += 1\n",
    "\n",
    "        # Update word_counts\n",
    "        new_word_counts = Counter()\n",
    "        for token_tuple_ints, count in word_counts.items():\n",
    "            updated_token_tuple_ints = _replace_pair_in_sequence(token_tuple_ints, best_pair_int_tuple, new_token_id)\n",
    "            new_word_counts[updated_token_tuple_ints] += count\n",
    "        word_counts = new_word_counts\n",
    "            \n",
    "        if len(vocab) >= vocab_size:\n",
    "            break\n",
    "        \n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_replace_pair_in_sequence test 1 PASSED.\n",
      "\n",
      "Testing full train_bpe with merge loop on: /var/folders/1_/__n8_ny14_g0tlyfd57xj1rw0000gn/T/tmp9sgrqr7f\n",
      "train_bpe (full test) took 0.0130 seconds.\n",
      "Learned vocab size: 258\n",
      "Number of merges: 2\n",
      "Learned merged tokens (IDs >= 256):\n",
      "  ID 256: b'ab' (decoded: 'ab')\n",
      "  ID 257: b'abab' (decoded: 'abab')\n",
      "Learned merges:\n",
      "  Merge 1: (b'a', b'b') -> (decoded: ('a', 'b'))\n",
      "  Merge 2: (b'ab', b'ab') -> (decoded: ('ab', 'ab'))\n",
      "First merge (a,b) is as expected.\n",
      "Second merge (ab,ab) is as expected.\n",
      "Cleaned up temporary file: /var/folders/1_/__n8_ny14_g0tlyfd57xj1rw0000gn/T/tmp9sgrqr7f\n",
      "\n",
      "Testing vocab_size assertion again:\n",
      "Caught expected assertion for small vocab_size (re-test): vocab_size (10) must be at least 257 to accommodate all unique special tokens and 256 base bytes.\n",
      "All train_bpe notebook tests (including merge loop basics) completed.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Example usage/test block for train_bpe, now including the merge loop.\n",
    "if __name__ == '__main__':\n",
    "    # --- Tests for _replace_pair_in_sequence ---\n",
    "    seq1 = (1, 2, 3, 1, 2, 4, 1, 2); pair1 = (1, 2); new_id1 = 99; expected1 = (99, 3, 99, 4, 99)\n",
    "    assert _replace_pair_in_sequence(seq1, pair1, new_id1) == expected1, \"_replace_pair_in_sequence test 1 FAILED\"\n",
    "    print(\"_replace_pair_in_sequence test 1 PASSED.\")\n",
    "\n",
    "    # --- Full train_bpe test with a small example ---\n",
    "    temp_file_path_full_test = \"\"\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as tmp_file:\n",
    "            tmp_file.write(\"abab aa ab\\n\") \n",
    "            tmp_file.write(\"abab cc ab\\n\") \n",
    "            temp_file_path_full_test = tmp_file.name\n",
    "        \n",
    "        print(f\"\\nTesting full train_bpe with merge loop on: {temp_file_path_full_test}\")\n",
    "        test_vocab_size = 258 \n",
    "        test_special_tokens = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        learned_vocab, learned_merges = train_bpe(temp_file_path_full_test, test_vocab_size, test_special_tokens)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"train_bpe (full test) took {end_time - start_time:.4f} seconds.\")\n",
    "        print(f\"Learned vocab size: {len(learned_vocab)}\")\n",
    "        print(f\"Number of merges: {len(learned_merges)}\")\n",
    "\n",
    "        print(\"Learned merged tokens (IDs >= 256):\")\n",
    "        for token_id, token_bytes in learned_vocab.items():\n",
    "            if token_id >= 256:\n",
    "                print(f\"  ID {token_id}: {token_bytes!r} (decoded: '{token_bytes.decode('utf-8', 'replace')}')\")\n",
    "        \n",
    "        print(\"Learned merges:\")\n",
    "        for m_idx, (p1, p2) in enumerate(learned_merges):\n",
    "            print(f\"  Merge {m_idx+1}: ({p1!r}, {p2!r}) -> (decoded: ('{p1.decode('utf-8', 'replace')}', '{p2.decode('utf-8', 'replace')}'))\")\n",
    "\n",
    "        assert len(learned_vocab) <= test_vocab_size, \"Vocab size exceeds target\"\n",
    "        # initial vocab for no special tokens is 256.\n",
    "        # num_merges_needed = 258 - 256 = 2\n",
    "        if test_vocab_size > 256: # Only assert if merges were expected\n",
    "            assert len(learned_merges) == test_vocab_size - 256, f\"Number of merges is incorrect. Expected {test_vocab_size - 256}, Got {len(learned_merges)}\"\n",
    "        \n",
    "        if learned_merges:\n",
    "            assert learned_merges[0] == (b'a', b'b'), f\"First merge unexpected: {learned_merges[0]}\"\n",
    "            print(\"First merge (a,b) is as expected.\")\n",
    "            if len(learned_merges) > 1:\n",
    "                 assert learned_merges[1] == (b'ab', b'ab'), f\"Second merge unexpected: {learned_merges[1]}\"\n",
    "                 print(\"Second merge (ab,ab) is as expected.\")\n",
    "        else:\n",
    "            print(\"No merges learned, skipping specific merge assertions.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during full train_bpe test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if temp_file_path_full_test and os.path.exists(temp_file_path_full_test):\n",
    "            os.remove(temp_file_path_full_test)\n",
    "            print(f\"Cleaned up temporary file: {temp_file_path_full_test}\")\n",
    "    \n",
    "    temp_file_path_assert = \"\"\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as tmp_file:\n",
    "            tmp_file.write(\"text\")\n",
    "            temp_file_path_assert = tmp_file.name\n",
    "        print(\"\\nTesting vocab_size assertion again:\")\n",
    "        train_bpe(temp_file_path_assert, 10, [\"<|endoftext|>\"])\n",
    "        assert False, \"AssertionError for small vocab_size not raised (re-test)\"\n",
    "    except AssertionError as e:\n",
    "        print(f\"Caught expected assertion for small vocab_size (re-test): {e}\")\n",
    "    finally:\n",
    "        if temp_file_path_assert and os.path.exists(temp_file_path_assert):\n",
    "            os.remove(temp_file_path_assert)\n",
    "\n",
    "    print(\"All train_bpe notebook tests (including merge loop basics) completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
