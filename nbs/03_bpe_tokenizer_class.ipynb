{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Class\n",
    "\n",
    "> Implementation of the `Tokenizer` class, which encapsulates the BPE vocabulary and merge rules to provide encoding and decoding functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp bpe_tokenizer_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import regex as re\n",
    "import json\n",
    "import base64\n",
    "from typing import List, Dict, Tuple, Optional, Iterable, Iterator, Any, Pattern\n",
    "import os # For temp file creation in example\n",
    "import tempfile # For temp file creation in example\n",
    "\n",
    "# Attempt to import from local utils, will be available after nbdev_export\n",
    "try:\n",
    "    from .utils import load_tokenizer_components, save_tokenizer_components\n",
    "except ImportError:\n",
    "    # Fallback for direct notebook execution or if package not fully installed\n",
    "    try:\n",
    "        from bpe_tokenizer_scratch.utils import load_tokenizer_components, save_tokenizer_components\n",
    "    except ImportError:\n",
    "        # Define directly if utils not available (e.g. for isolated testing of this notebook cell)\n",
    "        # This is not ideal for production but helps in iterative development of notebooks\n",
    "        print(\"Warning: 'utils' module not found, defining load/save_tokenizer_components locally for Tokenizer.from_files and testing.\")\n",
    "        def save_tokenizer_components(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], vocab_filepath: str, merges_filepath: str):\n",
    "            vocab_to_save = {str(k): base64.b64encode(v).decode('ascii') for k, v in vocab.items()}\n",
    "            with open(vocab_filepath, 'w', encoding='utf-8') as f: json.dump(vocab_to_save, f, indent=2)\n",
    "            merges_to_save = [(base64.b64encode(p1).decode('ascii'), base64.b64encode(p2).decode('ascii')) for p1, p2 in merges]\n",
    "            with open(merges_filepath, 'w', encoding='utf-8') as f: json.dump(merges_to_save, f, indent=2)\n",
    "\n",
    "        def load_tokenizer_components(vocab_filepath: str, merges_filepath: str) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "            with open(vocab_filepath, 'r', encoding='utf-8') as f: vocab_json = json.load(f)\n",
    "            loaded_vocab = {int(k): base64.b64decode(v_str) for k, v_str in vocab_json.items()}\n",
    "            with open(merges_filepath, 'r', encoding='utf-8') as f: merges_json = json.load(f)\n",
    "            loaded_merges = [(base64.b64decode(p1_str), base64.b64decode(p2_str)) for p1_str, p2_str in merges_json]\n",
    "            return loaded_vocab, loaded_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Tokenizer:\n",
    "    \"\"\" A BPE Tokenizer that can encode text into token IDs and decode token IDs back to text.\n",
    "    \n",
    "    The Tokenizer is initialized with a vocabulary (mapping IDs to byte sequences) \n",
    "    and a list of merge rules (ordered pairs of byte sequences that were merged during training).\n",
    "    It also handles special tokens.\n",
    "    \"\"\"\n",
    "    PAT_REGEX: Pattern = re.compile(r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab: Dict[int, bytes], \n",
    "                 merges: List[Tuple[bytes, bytes]], \n",
    "                 special_tokens: Optional[List[str]] = None):\n",
    "        \"\"\"Initializes the Tokenizer.\n",
    "\n",
    "        Args:\n",
    "            vocab (Dict[int, bytes]): A dictionary mapping token IDs (int) to their byte representations (bytes).\n",
    "                                     This vocab should ideally come from `train_bpe` and include base bytes (0-255)\n",
    "                                     and any initial special tokens passed to `train_bpe`.\n",
    "            merges (List[Tuple[bytes, bytes]]): An ordered list of BPE merge rules. Each rule is a tuple of two\n",
    "                                                byte sequences that were merged. The order determines merge priority.\n",
    "            special_tokens (Optional[List[str]]): A list of special token strings (e.g., \"<|endoftext|>\"). \n",
    "                                                    These tokens are handled distinctly during encoding/decoding.\n",
    "                                                    If a special token's byte representation is already in `vocab`,\n",
    "                                                    its existing ID is used. Otherwise, a new ID is assigned and \n",
    "                                                    it's added to the vocabulary.\n",
    "        \"\"\"\n",
    "        self.vocab: Dict[int, bytes] = vocab.copy() \n",
    "        self.merges_map: Dict[Tuple[bytes, bytes], int] = {pair: i for i, pair in enumerate(merges)} \n",
    "\n",
    "        self.special_tokens_list: List[str] = []\n",
    "        if special_tokens is not None:\n",
    "            seen_st = set()\n",
    "            for st in special_tokens:\n",
    "                if st not in seen_st:\n",
    "                    self.special_tokens_list.append(st)\n",
    "                    seen_st.add(st)\n",
    "            \n",
    "        self.special_token_to_id: Dict[str, int] = {} \n",
    "        self.id_to_special_token: Dict[int, str] = {} \n",
    "\n",
    "        _temp_bytes_to_id: Dict[bytes, int] = {b: i for i, b in sorted(self.vocab.items(), key=lambda item: item[0])}\n",
    "        \n",
    "        max_internal_id = -1\n",
    "        if self.vocab: \n",
    "            max_internal_id = max((k for k in self.vocab.keys() if isinstance(k, int)), default=-1)\n",
    "        \n",
    "        current_max_id_for_new_specials = max_internal_id\n",
    "\n",
    "        for st_text in self.special_tokens_list:\n",
    "            st_bytes = st_text.encode(\"utf-8\")\n",
    "            if st_bytes in _temp_bytes_to_id:\n",
    "                existing_id = _temp_bytes_to_id[st_bytes]\n",
    "                self.special_token_to_id[st_text] = existing_id\n",
    "                self.id_to_special_token[existing_id] = st_text\n",
    "                self.vocab[existing_id] = st_bytes \n",
    "            else:\n",
    "                current_max_id_for_new_specials += 1\n",
    "                new_id = current_max_id_for_new_specials\n",
    "                \n",
    "                self.vocab[new_id] = st_bytes      \n",
    "                _temp_bytes_to_id[st_bytes] = new_id \n",
    "                \n",
    "                self.special_token_to_id[st_text] = new_id\n",
    "                self.id_to_special_token[new_id] = st_text\n",
    "        \n",
    "        self.bytes_to_id: Dict[bytes, int] = {b: i for i, b in self.vocab.items()}\n",
    "        \n",
    "        if self.special_tokens_list:\n",
    "            sorted_unique_special_tokens_for_regex = sorted(list(set(self.special_tokens_list)), key=len, reverse=True)\n",
    "            self.special_tokens_regex_pattern = f\"({'|'.join(map(re.escape, sorted_unique_special_tokens_for_regex))})\"\n",
    "            self.special_tokens_regex: Optional[Pattern] = re.compile(self.special_tokens_regex_pattern)\n",
    "        else:\n",
    "            self.special_tokens_regex: Optional[Pattern] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_files(cls, \n",
    "                   vocab_filepath: str, \n",
    "                   merges_filepath: str, \n",
    "                   special_tokens: Optional[List[str]] = None) -> 'Tokenizer':\n",
    "        \"\"\"Loads a Tokenizer from vocabulary and merges files.\n",
    "\n",
    "        Args:\n",
    "            vocab_filepath (str): Path to the JSON file containing the vocabulary.\n",
    "            merges_filepath (str): Path to the JSON file containing the merge rules.\n",
    "            special_tokens (Optional[List[str]]): A list of special token strings.\n",
    "\n",
    "        Returns:\n",
    "            Tokenizer: An instance of the Tokenizer class initialized with the loaded data.\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: If vocab_filepath or merges_filepath does not exist.\n",
    "            json.JSONDecodeError: If the files are not valid JSON.\n",
    "            TypeError: If the loaded data is not in the expected format.\n",
    "        \"\"\"\n",
    "        loaded_vocab: Dict[int, bytes]\n",
    "        loaded_merges: List[Tuple[bytes, bytes]]\n",
    "        \n",
    "        if 'load_tokenizer_components' in globals() and callable(globals()['load_tokenizer_components']):\n",
    "            loaded_vocab, loaded_merges = globals()['load_tokenizer_components'](vocab_filepath, merges_filepath)\n",
    "        else:\n",
    "            try:\n",
    "                with open(vocab_filepath, 'r', encoding='utf-8') as f:\n",
    "                    vocab_json = json.load(f)\n",
    "                loaded_vocab = {int(k): base64.b64decode(v_str) for k, v_str in vocab_json.items()}\n",
    "\n",
    "                with open(merges_filepath, 'r', encoding='utf-8') as f:\n",
    "                    merges_json = json.load(f)\n",
    "                loaded_merges = [(base64.b64decode(p1_str), base64.b64decode(p2_str)) for p1_str, p2_str in merges_json]\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"Error: One or both tokenizer component files not found.\\nVocab: {vocab_filepath}\\nMerges: {merges_filepath}\")\n",
    "                raise e\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tokenizer components directly from files: {e}\")\n",
    "                raise e\n",
    "        \n",
    "        return cls(loaded_vocab, loaded_merges, special_tokens)\n",
    "\n",
    "    def _encode_bytes_with_merges(self, piece_bytes: bytes) -> List[int]:\n",
    "        \"\"\"Encodes a piece of bytes (typically a pre-tokenized word) using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            piece_bytes (bytes): The byte sequence of a pre-tokenized word.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: A list of token IDs representing the encoded piece.\n",
    "        \"\"\"\n",
    "        if not piece_bytes: \n",
    "            return []\n",
    "        \n",
    "        tokens_as_bytes_list: List[bytes] = [bytes([b]) for b in piece_bytes]\n",
    "\n",
    "        while len(tokens_as_bytes_list) > 1:\n",
    "            best_pair_info: Optional[Tuple[int, int]] = None \n",
    "            \n",
    "            for i in range(len(tokens_as_bytes_list) - 1):\n",
    "                pair = (tokens_as_bytes_list[i], tokens_as_bytes_list[i+1])\n",
    "                if pair in self.merges_map:\n",
    "                    rank = self.merges_map[pair]\n",
    "                    if best_pair_info is None or rank < best_pair_info[0]:\n",
    "                        best_pair_info = (rank, i)\n",
    "            \n",
    "            if best_pair_info is None:\n",
    "                break \n",
    "\n",
    "            _rank, idx = best_pair_info \n",
    "            merged_token_bytes = tokens_as_bytes_list[idx] + tokens_as_bytes_list[idx+1]\n",
    "            tokens_as_bytes_list = tokens_as_bytes_list[:idx] + [merged_token_bytes] + tokens_as_bytes_list[idx+2:]\n",
    "        \n",
    "        final_ids: List[int] = []\n",
    "        for b_sequence in tokens_as_bytes_list:\n",
    "            if b_sequence in self.bytes_to_id:\n",
    "                final_ids.append(self.bytes_to_id[b_sequence])\n",
    "            else:\n",
    "                # Fallback: if a multi-byte sequence is not found, encode its constituent bytes individually.\n",
    "                # This handles cases where a sequence (e.g., a single byte from an unknown character after pre-tokenization)\n",
    "                # was not part of any merge or the initial vocab explicitly, but its constituent bytes might be.\n",
    "                # This is crucial for ensuring that any arbitrary byte sequence can be tokenized at least into its base bytes.\n",
    "                # print(f\"Warning: Byte sequence {b_sequence!r} not in bytes_to_id map. Attempting to break down.\")\n",
    "                for single_byte_val in b_sequence: \n",
    "                    single_byte_obj = bytes([single_byte_val])\n",
    "                    if single_byte_obj in self.bytes_to_id:\n",
    "                         final_ids.append(self.bytes_to_id[single_byte_obj])\n",
    "                    else:\n",
    "                        # This should ideally not happen if the base 256 bytes are always in vocab.\n",
    "                        print(f\"Critical Warning: Single byte {single_byte_obj!r} (value: {single_byte_val}) not in bytes_to_id map. Vocab may be malformed or incomplete.\")\n",
    "                        # Options: raise error, assign a specific UNK_ID, or skip. Skipping for now based on prior context.\n",
    "                        pass \n",
    "        return final_ids\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encodes a string of text into a list of token IDs.\n",
    "\n",
    "        The encoding process involves:\n",
    "        1. Splitting the text by special tokens (if any are defined and present).\n",
    "        2. For segments that are special tokens, their pre-assigned IDs are used.\n",
    "        3. For other segments, GPT-2's pre-tokenization regex (PAT_REGEX) is applied to get \"words\".\n",
    "        4. Each \"word\" is converted to UTF-8 bytes.\n",
    "        5. BPE merges are applied to these bytes via `_encode_bytes_with_merges`.\n",
    "        6. All resulting token IDs are concatenated.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input string to encode.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: A list of token IDs representing the encoded text.\n",
    "        \"\"\"\n",
    "        token_ids: List[int] = []\n",
    "        \n",
    "        if not text: # Handle empty string input immediately\n",
    "            return []\n",
    "\n",
    "        # Determine segments to process: either the whole text or parts split by special tokens\n",
    "        segments_to_process: List[str]\n",
    "        if not self.special_tokens_regex: # No special tokens defined or no regex compiled\n",
    "            segments_to_process = [text]\n",
    "        else:\n",
    "            # Split text by special tokens, keeping the delimiters (special tokens themselves)\n",
    "            # Example: text=\"a<EOS>b\", regex capturing group for \"<EOS>\" -> parts=[\"a\", \"<EOS>\", \"b\"]\n",
    "            segments_to_process = self.special_tokens_regex.split(text)\n",
    "\n",
    "        for segment in segments_to_process:\n",
    "            if not segment: # re.split can produce empty strings if delimiters are adjacent or at ends\n",
    "                continue\n",
    "\n",
    "            if segment in self.special_token_to_id:\n",
    "                # This segment is a known special token\n",
    "                token_ids.append(self.special_token_to_id[segment])\n",
    "            else:\n",
    "                # This segment is not a special token, so pre-tokenize it using PAT_REGEX\n",
    "                # and then apply BPE merges to each pre-token.\n",
    "                for pre_token_match in self.PAT_REGEX.finditer(segment):\n",
    "                    pre_token_str = pre_token_match.group(0)\n",
    "                    pre_token_bytes = pre_token_str.encode(\"utf-8\") # Convert pre-token to bytes\n",
    "                    # Encode these bytes using BPE merges\n",
    "                    token_ids.extend(self._encode_bytes_with_merges(pre_token_bytes))\n",
    "        return token_ids\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        \"\"\"Encodes an iterable of text strings, yielding token IDs.\n",
    "\n",
    "        This method processes each string from the iterable independently and yields \n",
    "        its token IDs. It's suitable for tokenizing large collections of texts that \n",
    "        might not fit into memory if processed all at once as a single string.\n",
    "\n",
    "        Args:\n",
    "            iterable (Iterable[str]): An iterable (e.g., a list or generator) of strings to encode.\n",
    "\n",
    "        Yields:\n",
    "            int: Token IDs from the encoded text chunks.\n",
    "        \"\"\"\n",
    "        for text_chunk in iterable:\n",
    "            for token_id in self.encode(text_chunk):\n",
    "                yield token_id\n",
    "                \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Decodes a list of token IDs back into a string.\n",
    "\n",
    "        The decoding process involves:\n",
    "        1. Converting each token ID to its byte sequence using the vocabulary.\n",
    "           - Special token IDs are mapped back to their original string form (e.g., \"<EOS>\"), \n",
    "             then encoded to UTF-8 bytes.\n",
    "           - Regular token IDs are mapped directly to their byte sequences from the vocab.\n",
    "           - Unknown token IDs are skipped (as per project spec).\n",
    "        2. Concatenating all byte sequences.\n",
    "        3. Decoding the full byte sequence from UTF-8 to a string, using 'replace' for errors.\n",
    "\n",
    "        Args:\n",
    "            ids (List[int]): A list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        byte_chunks: List[bytes] = []\n",
    "        for token_id in ids:\n",
    "            if token_id in self.id_to_special_token:\n",
    "                # Special tokens are stored as strings in id_to_special_token;\n",
    "                # encode them back to bytes for consistent concatenation.\n",
    "                byte_chunks.append(self.id_to_special_token[token_id].encode('utf-8'))\n",
    "            elif token_id in self.vocab:\n",
    "                byte_chunks.append(self.vocab[token_id])\n",
    "            else:\n",
    "                # Behavior for unknown ID: Project request implies skipping.\n",
    "                # A warning could be logged here if desired for debugging.\n",
    "                # print(f\"Warning: Unknown token ID {token_id} encountered during decode. Skipping.\")\n",
    "                pass \n",
    "        \n",
    "        full_byte_sequence = b\"\".join(byte_chunks)\n",
    "        # Decode the full byte sequence, replacing any malformed UTF-8 sequences.\n",
    "        return full_byte_sequence.decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Example to test methods including encode, encode_iterable, and decode\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Setting up Tokenizer for tests ---\")\n",
    "\n",
    "    vocab_for_tests = {\n",
    "        **{i: bytes([i]) for i in range(256)},\n",
    "        ord('h'): ord('h'), ord('e'): ord('e'), ord('l'): ord('l'), ord('o'): ord('o'), \n",
    "        ord(' '): ord(' '), ord('w'): ord('w'), ord('r'): ord('r'), ord('d'): ord('d'),\n",
    "        ord('!'): ord('!'),\n",
    "        256: b'\\xe2\\x82\\xac', # Euro sign: €\n",
    "        257: b'ell',\n",
    "        258: b'ello',\n",
    "        259: b' wo',\n",
    "        260: b' wor',\n",
    "        261: b'orld',\n",
    "        262: b' world'\n",
    "    }\n",
    "    eos_bytes = b\"<EOS>\"\n",
    "    pad_bytes = b\"<PAD>\"\n",
    "\n",
    "    merges_for_tests = [\n",
    "        (b'e', b'l'), (b'el', b'l'), (b'ell', b'o'), (b' ', b'w'), (b' w', b'o'), \n",
    "        (b' wo', b'r'), (b'o', b'r'), (b'or', b'l'), (b'orl', b'd'), (b' wor', b'ld')\n",
    "    ]\n",
    "    special_tokens_for_tests = [\"<EOS>\", \"<PAD>\"]\n",
    "    \n",
    "    tokenizer_for_tests = Tokenizer(vocab_for_tests.copy(), merges_for_tests, special_tokens_for_tests)\n",
    "    \n",
    "    # IDs for special tokens will be assigned by __init__\n",
    "    eos_id = tokenizer_for_tests.special_token_to_id[\"<EOS>\"]\n",
    "    pad_id = tokenizer_for_tests.special_token_to_id[\"<PAD>\"]\n",
    "    print(f\"Assigned <EOS> ID: {eos_id}, <PAD> ID: {pad_id}\") # Should be 263, 264\n",
    "\n",
    "    print(\"\\n--- Testing decode method ---\")\n",
    "    decode_test_cases = {\n",
    "        \"simple sequence\": ([ord('h'), 258, 262, ord('!')], \"hello world!\"),\n",
    "        \"sequence with special token\": ([ord('h'), 258, eos_id, 262], \"hello<EOS> world\"),\n",
    "        \"empty list\": ([], \"\"),\n",
    "        \"unknown token ID\": ([ord('h'), 258, 999, 262], \"hello world\"), # 999 is unknown, should be skipped\n",
    "        \"only special tokens\": ([eos_id, pad_id, eos_id], \"<EOS><PAD><EOS>\"),\n",
    "        \"unicode character from vocab\": ([256], \"€\")\n",
    "    }\n",
    "    for name, (input_ids, expected_text) in decode_test_cases.items():\n",
    "        try:\n",
    "            result_text = tokenizer_for_tests.decode(input_ids)\n",
    "            assert result_text == expected_text, f\"Decode Test '{name}' FAILED. Expected '{expected_text}', Got '{result_text}' for IDs {input_ids}\"\n",
    "            print(f\"Decode Test '{name}' PASSED.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Decode Test '{name}' ERRORED for IDs {input_ids}: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "\n",
    "    print(\"\\n--- Testing encode/decode roundtrip ---\")\n",
    "    roundtrip_test_texts = [\n",
    "        \"hello world!\", \n",
    "        \"<EOS>hello<PAD> world€!\", \n",
    "        \"复杂的中文字符串\", \n",
    "        \"\",\n",
    "        \"   leading and trailing spaces   \",\n",
    "        \"'s 'd 'll 've 're 'm 't\",\n",
    "        \"A single byte like z then a special <EOS> token.\"\n",
    "    ]\n",
    "    for i, text in enumerate(roundtrip_test_texts):\n",
    "        try:\n",
    "            encoded_ids = tokenizer_for_tests.encode(text)\n",
    "            decoded_text = tokenizer_for_tests.decode(encoded_ids)\n",
    "            assert decoded_text == text, f\"Roundtrip Test {i} FAILED for '{text}'. Encoded: {encoded_ids}, Decoded: '{decoded_text}'\"\n",
    "            print(f\"Roundtrip Test {i} for '{text}' PASSED.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Roundtrip Test {i} for '{text}' ERRORED: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "\n",
    "    print(\"\\n--- Testing decode with malformed UTF-8 ---\")\n",
    "    # Create a temporary tokenizer and add a malformed UTF-8 sequence to its vocab\n",
    "    temp_vocab_malformed = tokenizer_for_tests.vocab.copy()\n",
    "    malformed_id = 1000 \n",
    "    temp_vocab_malformed[malformed_id] = b'\\xff\\xfe' # Invalid UTF-8 sequence\n",
    "    tokenizer_malformed_test = Tokenizer(temp_vocab_malformed, merges_for_tests, special_tokens_for_tests)\n",
    "    \n",
    "    ids_with_malformed = [ord('h'), malformed_id, ord('i')]\n",
    "    expected_malformed_decode = \"h\\ufffd\\ufffdi\" # Two replacement characters for b'\\xff\\xfe'\n",
    "    decoded_malformed_text = tokenizer_malformed_test.decode(ids_with_malformed)\n",
    "    assert decoded_malformed_text == expected_malformed_decode, \\\n",
    "        f\"Decode malformed UTF-8 FAILED. Expected '{expected_malformed_decode}', Got '{decoded_malformed_text}'\"\n",
    "    print(f\"Decode malformed UTF-8 PASSED. Decoded: '{decoded_malformed_text!r}'\")\n",
    "\n",
    "    print(\"\\nAll Tokenizer class method tests (init, from_files, _encode_bytes, encode, encode_iterable, decode) completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}