{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Class\n",
    "\n",
    "> Implementation of the `Tokenizer` class, which encapsulates the BPE vocabulary and merge rules to provide encoding and decoding functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp bpe_tokenizer_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# --- Project Root Setup --- \n",
    "project_root = Path(os.getcwd())\n",
    "# Check if running from nbs/\n",
    "if project_root.name == 'nbs' and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "elif not (project_root / 'settings.ini').exists():\n",
    "     # Try going up one level if settings.ini not found directly\n",
    "     if (project_root.parent / 'settings.ini').exists():\n",
    "          project_root = project_root.parent\n",
    "     else:\n",
    "          print(\"Warning: Could not automatically determine project root based on 'settings.ini'. Assuming current dir or its parent might be it, or sys.path is already correct.\")\n",
    "          # Fallback: Assume running from project root if structure unknown\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "if project_root_str not in sys.path:\n",
    "    # print(f\"Adding project root to sys.path: {project_root_str}\") # Verbose option\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    # print(f\"Project root already in sys.path: {project_root_str}\") # Less verbose\n",
    "    pass\n",
    "# --- End Project Root Setup --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import regex as re\n",
    "import json\n",
    "import base64\n",
    "from typing import List, Dict, Tuple, Optional, Iterable, Iterator, Any, Pattern\n",
    "import os # For temp file creation in example\n",
    "import tempfile # For temp file creation in example\n",
    "\n",
    "# Attempt to import from the installed/exported library first\n",
    "try:\n",
    "    from lm.utils import load_tokenizer_components, save_tokenizer_components\n",
    "except ImportError:\n",
    "    # Fallback for direct notebook execution if 'bpe_tokenizer_scratch' isn't in sys.path yet\n",
    "    # (sys.path modification already attempted in the cell above)\n",
    "    try:\n",
    "        from lm.utils import load_tokenizer_components, save_tokenizer_components\n",
    "        # print(\"Successfully imported from bpe_tokenizer_scratch.utils after sys.path check/modification.\")\n",
    "    except ImportError:\n",
    "        # This is the final fallback if the import still fails.\n",
    "        print(\"Warning: 'bpe_tokenizer_scratch.utils' module not found even after attempting sys.path modification. \\n\"\n",
    "              \"Defining load/save_tokenizer_components locally. This might happen if: \\n\"\n",
    "              \"  1. '00_utils.ipynb' has not been exported via `nbdev_export`.\\n\"\n",
    "              \"  2. The project structure differs from the assumed layout (e.g., not in 'nbs/' subdirectory). \\n\"\n",
    "              \"  3. The library name in 'settings.ini' (lib_name) is not 'bpe_tokenizer_scratch'.\\n\"\n",
    "              \"Ensure the package is correctly structured and exported, or install it via 'pip install -e .'.\")\n",
    "        \n",
    "        # Fallback: Define directly if utils not available\n",
    "        def save_tokenizer_components(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], vocab_filepath: str, merges_filepath: str):\n",
    "            vocab_to_save = {str(k): base64.b64encode(v).decode('ascii') for k, v in vocab.items()}\n",
    "            with open(vocab_filepath, 'w', encoding='utf-8') as f: json.dump(vocab_to_save, f, indent=2)\n",
    "            merges_to_save = [(base64.b64encode(p1).decode('ascii'), base64.b64encode(p2).decode('ascii')) for p1, p2 in merges]\n",
    "            with open(merges_filepath, 'w', encoding='utf-8') as f: json.dump(merges_to_save, f, indent=2)\n",
    "\n",
    "        def load_tokenizer_components(vocab_filepath: str, merges_filepath: str) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "            with open(vocab_filepath, 'r', encoding='utf-8') as f: vocab_json = json.load(f)\n",
    "            loaded_vocab = {int(k): base64.b64decode(v_str) for k, v_str in vocab_json.items()}\n",
    "            with open(merges_filepath, 'r', encoding='utf-8') as f: merges_json = json.load(f)\n",
    "            loaded_merges = [(base64.b64decode(p1_str), base64.b64decode(p2_str)) for p1_str, p2_str in merges_json]\n",
    "            return loaded_vocab, loaded_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Tokenizer:\n",
    "    \"\"\" A BPE Tokenizer that can encode text into token IDs and decode token IDs back to text.\n",
    "    \n",
    "    The Tokenizer is initialized with a vocabulary (mapping IDs to byte sequences) \n",
    "    and a list of merge rules (ordered pairs of byte sequences that were merged during training).\n",
    "    It also handles special tokens.\n",
    "    \"\"\"\n",
    "    PAT_REGEX: Pattern = re.compile(r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab: Dict[int, bytes], \n",
    "                 merges: List[Tuple[bytes, bytes]], \n",
    "                 special_tokens: Optional[List[str]] = None):\n",
    "        \"\"\"Initializes the Tokenizer.\n",
    "\n",
    "        Args:\n",
    "            vocab (Dict[int, bytes]): A dictionary mapping token IDs (int) to their byte representations (bytes).\n",
    "                                     This vocab should ideally come from `train_bpe` and include base bytes (0-255)\n",
    "                                     and any initial special tokens passed to `train_bpe`.\n",
    "            merges (List[Tuple[bytes, bytes]]): An ordered list of BPE merge rules. Each rule is a tuple of two\n",
    "                                                byte sequences that were merged. The order determines merge priority.\n",
    "            special_tokens (Optional[List[str]]): A list of special token strings (e.g., \"<|endoftext|>\"). \n",
    "                                                    These tokens are handled distinctly during encoding/decoding.\n",
    "                                                    If a special token's byte representation is already in `vocab`,\n",
    "                                                    its existing ID is used. Otherwise, a new ID is assigned and \n",
    "                                                    it's added to the vocabulary.\n",
    "        \"\"\"\n",
    "        self.vocab: Dict[int, bytes] = vocab.copy() \n",
    "        self.merges_map: Dict[Tuple[bytes, bytes], int] = {pair: i for i, pair in enumerate(merges)} \n",
    "\n",
    "        self.special_tokens_list: List[str] = []\n",
    "        if special_tokens is not None:\n",
    "            seen_st = set()\n",
    "            for st in special_tokens:\n",
    "                if st not in seen_st:\n",
    "                    self.special_tokens_list.append(st)\n",
    "                    seen_st.add(st)\n",
    "            \n",
    "        self.special_token_to_id: Dict[str, int] = {} \n",
    "        self.id_to_special_token: Dict[int, str] = {} \n",
    "\n",
    "        _temp_bytes_to_id: Dict[bytes, int] = {b: i for i, b in sorted(self.vocab.items(), key=lambda item: item[0])}\n",
    "        \n",
    "        max_internal_id = -1\n",
    "        if self.vocab: \n",
    "            max_internal_id = max((k for k in self.vocab.keys() if isinstance(k, int)), default=-1)\n",
    "        \n",
    "        current_max_id_for_new_specials = max_internal_id\n",
    "\n",
    "        for st_text in self.special_tokens_list:\n",
    "            st_bytes = st_text.encode(\"utf-8\")\n",
    "            if st_bytes in _temp_bytes_to_id:\n",
    "                existing_id = _temp_bytes_to_id[st_bytes]\n",
    "                self.special_token_to_id[st_text] = existing_id\n",
    "                self.id_to_special_token[existing_id] = st_text\n",
    "                self.vocab[existing_id] = st_bytes \n",
    "            else:\n",
    "                current_max_id_for_new_specials += 1\n",
    "                new_id = current_max_id_for_new_specials\n",
    "                \n",
    "                self.vocab[new_id] = st_bytes      \n",
    "                _temp_bytes_to_id[st_bytes] = new_id \n",
    "                \n",
    "                self.special_token_to_id[st_text] = new_id\n",
    "                self.id_to_special_token[new_id] = st_text\n",
    "        \n",
    "        self.bytes_to_id: Dict[bytes, int] = {b: i for i, b in self.vocab.items()}\n",
    "        \n",
    "        if self.special_tokens_list:\n",
    "            sorted_unique_special_tokens_for_regex = sorted(list(set(self.special_tokens_list)), key=len, reverse=True)\n",
    "            self.special_tokens_regex_pattern = f\"({'|'.join(map(re.escape, sorted_unique_special_tokens_for_regex))})\"\n",
    "            self.special_tokens_regex: Optional[Pattern] = re.compile(self.special_tokens_regex_pattern)\n",
    "        else:\n",
    "            self.special_tokens_regex: Optional[Pattern] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_files(cls, \n",
    "                   vocab_filepath: str, \n",
    "                   merges_filepath: str, \n",
    "                   special_tokens: Optional[List[str]] = None) -> 'Tokenizer':\n",
    "        \"\"\"Loads a Tokenizer from vocabulary and merges files.\n",
    "\n",
    "        Args:\n",
    "            vocab_filepath (str): Path to the JSON file containing the vocabulary.\n",
    "            merges_filepath (str): Path to the JSON file containing the merge rules.\n",
    "            special_tokens (Optional[List[str]]): A list of special token strings.\n",
    "\n",
    "        Returns:\n",
    "            Tokenizer: An instance of the Tokenizer class initialized with the loaded data.\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: If vocab_filepath or merges_filepath does not exist.\n",
    "            json.JSONDecodeError: If the files are not valid JSON.\n",
    "            TypeError: If the loaded data is not in the expected format.\n",
    "        \"\"\"\n",
    "        loaded_vocab: Dict[int, bytes]\n",
    "        loaded_merges: List[Tuple[bytes, bytes]]\n",
    "        \n",
    "        if 'load_tokenizer_components' in globals() and callable(globals()['load_tokenizer_components']):\n",
    "            try:\n",
    "                 loaded_vocab, loaded_merges = globals()['load_tokenizer_components'](vocab_filepath, merges_filepath)\n",
    "            except Exception as e:\n",
    "                print(f\"Error using 'load_tokenizer_components' (potentially from utils.py): {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(\"Critical: 'load_tokenizer_components' is not defined or not callable. Cannot load from files.\")\n",
    "            raise NameError(\"Helper function 'load_tokenizer_components' not available.\")\n",
    "        \n",
    "        return cls(loaded_vocab, loaded_merges, special_tokens)\n",
    "\n",
    "    def _encode_bytes_with_merges(self, piece_bytes: bytes) -> List[int]:\n",
    "        \"\"\"Encodes a piece of bytes (typically a pre-tokenized word) using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            piece_bytes (bytes): The byte sequence of a pre-tokenized word.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: A list of token IDs representing the encoded piece.\n",
    "        \"\"\"\n",
    "        if not piece_bytes: \n",
    "            return []\n",
    "        \n",
    "        tokens_as_bytes_list: List[bytes] = [bytes([b]) for b in piece_bytes]\n",
    "\n",
    "        while len(tokens_as_bytes_list) > 1:\n",
    "            best_pair_info: Optional[Tuple[int, int]] = None \n",
    "            \n",
    "            for i in range(len(tokens_as_bytes_list) - 1):\n",
    "                pair = (tokens_as_bytes_list[i], tokens_as_bytes_list[i+1])\n",
    "                if pair in self.merges_map:\n",
    "                    rank = self.merges_map[pair]\n",
    "                    if best_pair_info is None or rank < best_pair_info[0]:\n",
    "                        best_pair_info = (rank, i)\n",
    "            \n",
    "            if best_pair_info is None:\n",
    "                break \n",
    "\n",
    "            _rank, idx = best_pair_info \n",
    "            merged_token_bytes = tokens_as_bytes_list[idx] + tokens_as_bytes_list[idx+1]\n",
    "            tokens_as_bytes_list = tokens_as_bytes_list[:idx] + [merged_token_bytes] + tokens_as_bytes_list[idx+2:]\n",
    "        \n",
    "        final_ids: List[int] = []\n",
    "        for b_sequence in tokens_as_bytes_list:\n",
    "            if b_sequence in self.bytes_to_id:\n",
    "                final_ids.append(self.bytes_to_id[b_sequence])\n",
    "            else:\n",
    "                for single_byte_val in b_sequence: \n",
    "                    single_byte_obj = bytes([single_byte_val])\n",
    "                    if single_byte_obj in self.bytes_to_id:\n",
    "                         final_ids.append(self.bytes_to_id[single_byte_obj])\n",
    "                    else:\n",
    "                        print(f\"Critical Warning: Single byte {single_byte_obj!r} (value: {single_byte_val}) not in bytes_to_id map. Vocab may be malformed or incomplete.\")\n",
    "                        pass \n",
    "        return final_ids\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encodes a string of text into a list of token IDs.\n",
    "\n",
    "        The encoding process involves:\n",
    "        1. Splitting the text by special tokens (if any are defined and present).\n",
    "        2. For segments that are special tokens, their pre-assigned IDs are used.\n",
    "        3. For other segments, GPT-2's pre-tokenization regex (PAT_REGEX) is applied to get \"words\".\n",
    "        4. Each \"word\" is converted to UTF-8 bytes.\n",
    "        5. BPE merges are applied to these bytes via `_encode_bytes_with_merges`.\n",
    "        6. All resulting token IDs are concatenated.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input string to encode.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: A list of token IDs representing the encoded text.\n",
    "        \"\"\"\n",
    "        token_ids: List[int] = []\n",
    "        \n",
    "        if not text: # Handle empty string input immediately\n",
    "            return []\n",
    "\n",
    "        segments_to_process: List[str]\n",
    "        if not self.special_tokens_regex: \n",
    "            segments_to_process = [text]\n",
    "        else:\n",
    "            segments_to_process = self.special_tokens_regex.split(text)\n",
    "\n",
    "        for segment in segments_to_process:\n",
    "            if not segment: \n",
    "                continue\n",
    "\n",
    "            if segment in self.special_token_to_id:\n",
    "                token_ids.append(self.special_token_to_id[segment])\n",
    "            else:\n",
    "                for pre_token_match in self.PAT_REGEX.finditer(segment):\n",
    "                    pre_token_str = pre_token_match.group(0)\n",
    "                    pre_token_bytes = pre_token_str.encode(\"utf-8\") \n",
    "                    token_ids.extend(self._encode_bytes_with_merges(pre_token_bytes))\n",
    "        return token_ids\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        \"\"\"Encodes an iterable of text strings, yielding token IDs.\n",
    "\n",
    "        Args:\n",
    "            iterable (Iterable[str]): An iterable (e.g., a list or generator) of strings to encode.\n",
    "\n",
    "        Yields:\n",
    "            int: Token IDs from the encoded text chunks.\n",
    "        \"\"\"\n",
    "        for text_chunk in iterable:\n",
    "            for token_id in self.encode(text_chunk):\n",
    "                yield token_id\n",
    "                \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Decodes a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            ids (List[int]): A list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        byte_chunks: List[bytes] = []\n",
    "        for token_id in ids:\n",
    "            if token_id in self.id_to_special_token:\n",
    "                byte_chunks.append(self.id_to_special_token[token_id].encode('utf-8'))\n",
    "            elif token_id in self.vocab:\n",
    "                # Ensure that self.vocab[token_id] is indeed bytes\n",
    "                token_val = self.vocab[token_id]\n",
    "                if isinstance(token_val, bytes):\n",
    "                    byte_chunks.append(token_val)\n",
    "                else:\n",
    "                    # This case should not happen if vocab is constructed correctly\n",
    "                    print(f\"Warning: Vocab item for ID {token_id} is not bytes: {token_val!r}. Skipping.\")\n",
    "                    pass \n",
    "            else:\n",
    "                # print(f\"Warning: Unknown token ID {token_id} encountered during decode. Skipping.\")\n",
    "                pass \n",
    "        \n",
    "        full_byte_sequence = b\"\".join(byte_chunks)\n",
    "        return full_byte_sequence.decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up Tokenizer for tests ---\n",
      "Assigned <EOS> ID: 263, <PAD> ID: 264\n",
      "\n",
      "--- Testing decode method ---\n",
      "Decode Test 'simple sequence' PASSED.\n",
      "Decode Test 'sequence with special token' PASSED.\n",
      "Decode Test 'empty list' PASSED.\n",
      "Decode Test 'unknown token ID' PASSED.\n",
      "Decode Test 'only special tokens' PASSED.\n",
      "Decode Test 'unicode character from vocab' PASSED.\n",
      "\n",
      "--- Testing encode/decode roundtrip ---\n",
      "Roundtrip Test 0 for 'hello world!' PASSED.\n",
      "Roundtrip Test 1 for '<EOS>hello<PAD> world€!' PASSED.\n",
      "Roundtrip Test 2 for '复杂的中文字符串' PASSED.\n",
      "Roundtrip Test 3 for '' PASSED.\n",
      "Roundtrip Test 4 for '   leading and trailing spaces   ' PASSED.\n",
      "Roundtrip Test 5 for ''s 'd 'll 've 're 'm 't' PASSED.\n",
      "Roundtrip Test 6 for 'A single byte like z then a special <EOS> token.' PASSED.\n",
      "\n",
      "--- Testing decode with malformed UTF-8 ---\n",
      "Decode malformed UTF-8 PASSED. Decoded: ''h��i''\n",
      "\n",
      "All Tokenizer class method tests (init, from_files, _encode_bytes, encode, encode_iterable, decode) completed.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Example to test methods including encode, encode_iterable, and decode\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Setting up Tokenizer for tests ---\")\n",
    "\n",
    "    # Corrected vocab_for_tests: Values must be bytes.\n",
    "    # IDs 0-255 map to their respective single byte representations.\n",
    "    # Merged tokens use IDs >= 256.\n",
    "    vocab_for_tests = {\n",
    "        **{i: bytes([i]) for i in range(256)}, \n",
    "        # Merged token IDs and their byte sequences:\n",
    "        256: b'\\xe2\\x82\\xac', # Euro sign: €\n",
    "        257: b'ell',         # Represents the merge of 'e' and 'll' (hypothetically)\n",
    "        258: b'ello',        # Represents the merge of 'ell' and 'o'\n",
    "        259: b' wo',         # Represents ' ' and 'w' merged, then with 'o'\n",
    "        260: b' wor',        # Represents ' wo' and 'r'\n",
    "        261: b'orld',        # Represents 'orl' and 'd'\n",
    "        262: b' world'       # Represents ' wor' and 'ld'\n",
    "    }\n",
    "\n",
    "    merges_for_tests = [\n",
    "        (b'e', b'l'),      # rank 0\n",
    "        (b'el', b'l'),     # rank 1 -> b'ell' (ID 257)\n",
    "        (b'ell', b'o'),    # rank 2 -> b'ello' (ID 258)\n",
    "        (b' ', b'w'),      # rank 3\n",
    "        (b' w', b'o'),     # rank 4 -> b' wo' (ID 259)\n",
    "        (b' wo', b'r'),    # rank 5 -> b' wor' (ID 260)\n",
    "        (b'o', b'r'),      # rank 6\n",
    "        (b'or', b'l'),     # rank 7\n",
    "        (b'orl', b'd'),    # rank 8 -> b'orld' (ID 261)\n",
    "        # Example: (b' wor', b'ld') -> b' world' (ID 262) is rank 9 (if b'ld' is formed first)\n",
    "        # For simplicity of testing, let's assume the merges directly result in the tokens in vocab.\n",
    "        # The example vocab implies b'orld' is a token, and b' world' is also a token.\n",
    "        # Let's add a merge that could form b' world' if b'ld' was also a merged token\n",
    "        (b' wor', bytes([108, 100])) # Example for (b' wor', b'ld') -> Not used if b'ld' isn't a token itself\n",
    "    ]\n",
    "    # Ensure merges_for_tests only contains pairs that would form tokens present in vocab_for_tests or lead to them.\n",
    "    # For this test, we care more that the merges_map is built correctly and applied.\n",
    "\n",
    "    special_tokens_for_tests = [\"<EOS>\", \"<PAD>\"]\n",
    "    \n",
    "    tokenizer_for_tests = Tokenizer(vocab_for_tests.copy(), merges_for_tests, special_tokens_for_tests)\n",
    "    \n",
    "    eos_id = tokenizer_for_tests.special_token_to_id[\"<EOS>\"] # Should be 263\n",
    "    pad_id = tokenizer_for_tests.special_token_to_id[\"<PAD>\"] # Should be 264\n",
    "    print(f\"Assigned <EOS> ID: {eos_id}, <PAD> ID: {pad_id}\")\n",
    "\n",
    "    print(\"\\n--- Testing decode method ---\")\n",
    "    decode_test_cases = {\n",
    "        \"simple sequence\": ([ord('h'), 258, 262, ord('!')], \"hello world!\"),\n",
    "        \"sequence with special token\": ([ord('h'), 258, eos_id, 262], \"hello<EOS> world\"),\n",
    "        \"empty list\": ([], \"\"),\n",
    "        \"unknown token ID\": ([ord('h'), 258, 999, 262], \"hello world\"), \n",
    "        \"only special tokens\": ([eos_id, pad_id, eos_id], \"<EOS><PAD><EOS>\"),\n",
    "        \"unicode character from vocab\": ([256], \"€\")\n",
    "    }\n",
    "    for name, (input_ids, expected_text) in decode_test_cases.items():\n",
    "        try:\n",
    "            result_text = tokenizer_for_tests.decode(input_ids)\n",
    "            assert result_text == expected_text, f\"Decode Test '{name}' FAILED. Expected '{expected_text}', Got '{result_text}' for IDs {input_ids}\"\n",
    "            print(f\"Decode Test '{name}' PASSED.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Decode Test '{name}' ERRORED for IDs {input_ids}: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "\n",
    "    print(\"\\n--- Testing encode/decode roundtrip ---\")\n",
    "    roundtrip_test_texts = [\n",
    "        \"hello world!\", \n",
    "        \"<EOS>hello<PAD> world€!\", \n",
    "        \"复杂的中文字符串\", \n",
    "        \"\",\n",
    "        \"   leading and trailing spaces   \",\n",
    "        \"'s 'd 'll 've 're 'm 't\",\n",
    "        \"A single byte like z then a special <EOS> token.\"\n",
    "    ]\n",
    "    for i, text in enumerate(roundtrip_test_texts):\n",
    "        try:\n",
    "            encoded_ids = tokenizer_for_tests.encode(text)\n",
    "            decoded_text = tokenizer_for_tests.decode(encoded_ids)\n",
    "            assert decoded_text == text, f\"Roundtrip Test {i} FAILED for '{text}'. Encoded: {encoded_ids}, Decoded: '{decoded_text}'\"\n",
    "            print(f\"Roundtrip Test {i} for '{text[:50]}' PASSED.\") # Truncate long text for printing\n",
    "        except Exception as e:\n",
    "            print(f\"Roundtrip Test {i} for '{text[:50]}' ERRORED: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "\n",
    "    print(\"\\n--- Testing decode with malformed UTF-8 ---\")\n",
    "    temp_vocab_malformed = tokenizer_for_tests.vocab.copy()\n",
    "    malformed_id = 1000 \n",
    "    temp_vocab_malformed[malformed_id] = b'\\xff\\xfe' # Invalid UTF-8 sequence\n",
    "    tokenizer_malformed_test = Tokenizer(temp_vocab_malformed, merges_for_tests, special_tokens_for_tests)\n",
    "    \n",
    "    ids_with_malformed = [ord('h'), malformed_id, ord('i')]\n",
    "    expected_malformed_decode = \"h\\ufffd\\ufffdi\" \n",
    "    decoded_malformed_text = tokenizer_malformed_test.decode(ids_with_malformed)\n",
    "    assert decoded_malformed_text == expected_malformed_decode, \\\n",
    "        f\"Decode malformed UTF-8 FAILED. Expected '{expected_malformed_decode}', Got '{decoded_malformed_text}'\"\n",
    "    print(f\"Decode malformed UTF-8 PASSED. Decoded: '{decoded_malformed_text!r}'\")\n",
    "\n",
    "    print(\"\\nAll Tokenizer class method tests (init, from_files, _encode_bytes, encode, encode_iterable, decode) completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
