# Autogenerated by nbdev

d = { 'settings': { 'branch': 'main',
                'doc_baseurl': '/lm',
                'doc_host': 'https://numb3r33.github.io',
                'git_url': 'https://github.com/numb3r33/lm',
                'lib_path': 'lm'},
  'syms': { 'lm.adapters': { 'lm.adapters.get_tokenizer_adapter': ('adapters.html#get_tokenizer_adapter', 'lm/adapters.py'),
                             'lm.adapters.get_tokenizer_from_files_adapter': ( 'adapters.html#get_tokenizer_from_files_adapter',
                                                                               'lm/adapters.py'),
                             'lm.adapters.run_train_bpe_adapter': ('adapters.html#run_train_bpe_adapter', 'lm/adapters.py')},
            'lm.bpe_tokenizer_class': { 'lm.bpe_tokenizer_class.Tokenizer': ( 'bpe_tokenizer_class.html#tokenizer',
                                                                              'lm/bpe_tokenizer_class.py'),
                                        'lm.bpe_tokenizer_class.Tokenizer.__init__': ( 'bpe_tokenizer_class.html#tokenizer.__init__',
                                                                                       'lm/bpe_tokenizer_class.py'),
                                        'lm.bpe_tokenizer_class.Tokenizer._encode_bytes_with_merges': ( 'bpe_tokenizer_class.html#tokenizer._encode_bytes_with_merges',
                                                                                                        'lm/bpe_tokenizer_class.py'),
                                        'lm.bpe_tokenizer_class.Tokenizer.decode': ( 'bpe_tokenizer_class.html#tokenizer.decode',
                                                                                     'lm/bpe_tokenizer_class.py'),
                                        'lm.bpe_tokenizer_class.Tokenizer.encode': ( 'bpe_tokenizer_class.html#tokenizer.encode',
                                                                                     'lm/bpe_tokenizer_class.py'),
                                        'lm.bpe_tokenizer_class.Tokenizer.encode_iterable': ( 'bpe_tokenizer_class.html#tokenizer.encode_iterable',
                                                                                              'lm/bpe_tokenizer_class.py'),
                                        'lm.bpe_tokenizer_class.Tokenizer.from_files': ( 'bpe_tokenizer_class.html#tokenizer.from_files',
                                                                                         'lm/bpe_tokenizer_class.py')},
            'lm.bpe_training': { 'lm.bpe_training._process_segment': ('bpe_training.html#_process_segment', 'lm/bpe_training.py'),
                                 'lm.bpe_training._replace_pair_in_sequence': ( 'bpe_training.html#_replace_pair_in_sequence',
                                                                                'lm/bpe_training.py'),
                                 'lm.bpe_training.train_bpe': ('bpe_training.html#train_bpe', 'lm/bpe_training.py')},
            'lm.core': {'lm.core.foo': ('core.html#foo', 'lm/core.py')},
            'lm.utils': { 'lm.utils.load_tokenizer_components': ('utils.html#load_tokenizer_components', 'lm/utils.py'),
                          'lm.utils.save_tokenizer_components': ('utils.html#save_tokenizer_components', 'lm/utils.py')}}}
