"""Utility functions for the BPE Tokenizer project, primarily for serialization/deserialization."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_utils.ipynb.

# %% auto 0
__all__ = ['save_tokenizer_components', 'load_tokenizer_components']

# %% ../nbs/00_utils.ipynb 3
import json
import base64
from typing import Dict, List, Tuple

def save_tokenizer_components(
    vocab: Dict[int, bytes], 
    merges: List[Tuple[bytes, bytes]], 
    vocab_filepath: str, 
    merges_filepath: str
):
    """Saves tokenizer vocab and merges to JSON files with Base64 encoding for bytes.

    Args:
        vocab (Dict[int, bytes]): Vocabulary mapping token IDs to byte sequences.
        merges (List[Tuple[bytes, bytes]]): List of merge rules (pairs of byte sequences).
        vocab_filepath (str): Path to save the vocabulary JSON file.
        merges_filepath (str): Path to save the merges JSON file.
    """
    # Convert vocab: keys to str, bytes values to base64 encoded ascii strings
    vocab_to_save = {str(k): base64.b64encode(v).decode('ascii') for k, v in vocab.items()}
    with open(vocab_filepath, 'w', encoding='utf-8') as f:
        json.dump(vocab_to_save, f, indent=2)

    # Convert merges: each byte sequence in pairs to base64 encoded ascii strings
    merges_to_save = [
        (base64.b64encode(p1).decode('ascii'), base64.b64encode(p2).decode('ascii')) 
        for p1, p2 in merges
    ]
    with open(merges_filepath, 'w', encoding='utf-8') as f:
        json.dump(merges_to_save, f, indent=2)

#| export
def load_tokenizer_components(
    vocab_filepath: str, 
    merges_filepath: str
) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:
    """Loads tokenizer vocab and merges from JSON files, decoding Base64 for bytes.

    Args:
        vocab_filepath (str): Path to the vocabulary JSON file.
        merges_filepath (str): Path to the merges JSON file.

    Returns:
        Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]: Loaded vocabulary and merges.
    """
    # Load vocab: convert str keys to int, base64 encoded ascii strings to bytes
    with open(vocab_filepath, 'r', encoding='utf-8') as f:
        vocab_json = json.load(f)
    loaded_vocab = {int(k): base64.b64decode(v_str) for k, v_str in vocab_json.items()}

    # Load merges: convert base64 encoded ascii strings in pairs back to bytes
    with open(merges_filepath, 'r', encoding='utf-8') as f:
        merges_json = json.load(f)
    loaded_merges = [
        (base64.b64decode(p1_str), base64.b64decode(p2_str)) 
        for p1_str, p2_str in merges_json
    ]
    
    return loaded_vocab, loaded_merges
